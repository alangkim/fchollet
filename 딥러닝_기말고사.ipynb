{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝_기말고사.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMmq8WTPQdNEFNcmrj1cczL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alangkim/fchollet/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch8. Introduction to deep learning for computer vision"
      ],
      "metadata": {
        "id": "95Z4hsdS-38b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction to\n",
        "convnets\n",
        "\n",
        "2. Training a\n",
        "convnet from scratch on a small dataset\n",
        "\n",
        "3. Leveraging a\n",
        "pretrained model"
      ],
      "metadata": {
        "id": "AwpvaKrb_AAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction to convnets"
      ],
      "metadata": {
        "id": "McKuibZi_Mfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stack of Conv2D and MaxPooling2D layers"
      ],
      "metadata": {
        "id": "qRT-9fvw_VmY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk2kADnJoxUw"
      },
      "outputs": [],
      "source": [
        "# Instantiating a small convnet\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(28, 28, 1))                                     # MNIST dataset을 이용하기 위해 28*28 사용\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)     # Conv2D\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)                                     # MaxPooling2D\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)                                                     # Flatten all the information\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)                         # connect Dense layer\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)                         # making model by functional API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZZyYWIxoxUw"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOxV0KpuoxUw"
      },
      "outputs": [],
      "source": [
        "# Training the convnet on MNIST images\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)) # CNN을 이용하기 위해서 channel dimension은 필수적이다.\n",
        "# Convnet is running on the original shape of the image.\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\", # multi class classification\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the convnet\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "z-wybeXvAIJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The convolution operation"
      ],
      "metadata": {
        "id": "dtZv-RvYrmcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 'Dense layers' learn 'global patterns' in their input feature space whereas 'convolution layers' learn 'local patterns'\n",
        "\n",
        "* The patterns they learn are\n",
        "translation invariant\n",
        "\n",
        "* They can learn spatial hierarchies of patterns\n",
        "\n",
        "* Convolution preserves the spatial relationship between pixels by learning image\n",
        "features using small squares (depending on the filter size) of input data\n",
        "\n",
        "* Convolution: multiplying elementwise by filter and summing the multiplication\n",
        "outputs\n",
        "\n",
        "* Ex) a 3x3 kernel or 3x3x1 filter acts on a 5x6 input image with stride 1 and outputs\n",
        "a 3x4 feature map.\n",
        "\n",
        "* In fully connected sense, we need unshared 30(=5x6)x12(=3x4) weights (input size x output size)\n",
        "\n",
        "* 9 vs 360. So using convolution filter is far more efficient."
      ],
      "metadata": {
        "id": "3ouJ_NL9rrh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution on MxNx3 image with 3x3x3 filter producing 1 feature map by taking dot products between the filter and 3x3x3 piecies of the image.\n",
        "\n",
        "Depth part is decided based on the input feature map."
      ],
      "metadata": {
        "id": "7hZiPhQpaxi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why convolution?"
      ],
      "metadata": {
        "id": "DEi2OSthb5ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fully Connected -> 1000x1000 images, 10000 hidden nodes, 10^10 parameters\n",
        "* Convolution     -> 1000x1000 images, 10x10 filter size, 100 filters, 10^4 parameters\n",
        "\n",
        "* If you are dealing with image dataset, it's highly recommend to use convolution layers in modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "my-gSQFccLmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How convolution filter works?"
      ],
      "metadata": {
        "id": "r7Tjwn0OeVh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different values of the filter matrix produce different\n",
        "feature maps for the same input image.\n",
        "\n",
        "CNN learns the values of filters during training\n",
        "\n",
        "The more filters, the more features are extracted"
      ],
      "metadata": {
        "id": "jFMAcI2_eHt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature map"
      ],
      "metadata": {
        "id": "TtSwIzOsXFCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4 parameters of feature map\n",
        "\n",
        "1. filter size\n",
        "2. depth\n",
        "3. stride\n",
        "4. zero-padding"
      ],
      "metadata": {
        "id": "zL5fMh7yXVBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The max pooling operation"
      ],
      "metadata": {
        "id": "eXn7q_J9XsfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Role\n",
        "of max pooling: to aggressively downsample feature maps\n",
        "\n",
        "Transformed via a hardcoded max\n",
        "tensor operation\n",
        "\n",
        "We need the features from the last\n",
        "convolution layer to contain\n",
        "information about the totality of the\n",
        "input\n",
        "\n",
        "The final feature map has 22\n",
        "× 22 ×\n",
        "128 = 61,952 total coefficients per\n",
        "sample\n",
        "\n",
        "This is far too large for such a\n",
        "small model and would result in\n",
        "intense overfitting"
      ],
      "metadata": {
        "id": "gVJW72urXwjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max-pooling이 없는 경우\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "j3p6KvfpAIFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool.summary()\n",
        "# 모델의 크기에 비해 parameters가 너무 많다."
      ],
      "metadata": {
        "id": "vcWDvOrWAIC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max-pooling은 없지만 stride를 2로 지정한 경우\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")(inputs) # stride = 2 로 지정.\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "QoaoW4dZAH8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool.summary()\n",
        "# parameters가 많이 줄어들었으나 max-pooling의 결과가 더 좋다.\n",
        "# 일반적으로 classification에서는 stride보다 max-pooling을 자주 사용한다.\n",
        "# 경험적으로 대부분 average-pooling보다 max-poolng이 좋다."
      ],
      "metadata": {
        "id": "NHP6j9KlAHr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training a convnet from scratch on a small dataset"
      ],
      "metadata": {
        "id": "gyYyyqIkhznz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading a\n",
        "Kaggle dataset in Google Colaboratory\n",
        "\n",
        "Access to the API is restricted to\n",
        "Kaggle users, you need to authenticate yourself.\n",
        "\n",
        "The\n",
        "kaggle package will look for your login credentials in a JSON file located at\n",
        "kaggle kaggle.json\n",
        "\n",
        "First, you need to create a\n",
        "Kaggle API key and download it to your local machine\n",
        "Login\n",
        "--> My Account --> Account settings --> API\n",
        "Click the Create New API Token\n",
        "button\n",
        "\n",
        "\n",
        "Second, go to your\n",
        "Colab notebook, and upload the API’s key JSON file to your\n",
        "Colab session by running the following code in a notebook cell:"
      ],
      "metadata": {
        "id": "PHjhFVBMh78x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러오기"
      ],
      "metadata": {
        "id": "LXeOrQMAu8M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "7yLglkdUh2AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "rpLHAqo7ioCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "metadata": {
        "id": "URLInUr9iyPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "Aoz3fYDht2G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq dogs-vs-cats.zip"
      ],
      "metadata": {
        "id": "WPYiVZdhuQeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "o4SpUBbXuhnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq train.zip"
      ],
      "metadata": {
        "id": "P7rI26zci60T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "W3c9kqftgHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('train')"
      ],
      "metadata": {
        "id": "TYgR2-MOurIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copying images to training, validation, and test directories"
      ],
      "metadata": {
        "id": "8f_Gm9WJvF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "복잡하게 나열되어있는 data를 train, validation, test로 나누고 각각 1000개, 500개, 1000개의 data를 넣는 전처리"
      ],
      "metadata": {
        "id": "biCIyvK-vQ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "# original dataset이 풀려있는 directory\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "# smaller dataset을 저장할 directory\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        # 새로운 directory 만들기 ex) cats_vs_dogs_small/train/dog\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        # 파일 이름 만들기\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "            # src : source, dst : destination\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "# 처음 1000개로 train set을 만듦\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "# 그 다음 500개로 validation set을 만듦\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)\n",
        "# 그 다음 1000개로 test set을 만듦"
      ],
      "metadata": {
        "id": "eqRdeLOwi9um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(new_base_dir)"
      ],
      "metadata": {
        "id": "7HQHyI4WgYm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 코드와 동일\n",
        "os.listdir('cats_vs_dogs_small')"
      ],
      "metadata": {
        "id": "wEcojIOcg5-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('cats_vs_dogs_small/test')"
      ],
      "metadata": {
        "id": "ukgcOCRGgmC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('cats_vs_dogs_small/test/dog')\n",
        "# 1500~2500 index를 가진 dog 파일이 들어가있음"
      ],
      "metadata": {
        "id": "V4YvE_GNgxoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "# 180x180 size를 가진 RGB image\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "# rescale\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "# binary classification이라 activation은 sigmoid\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "JL_OwU5yjMLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "# height, width는 점점 작아지고 depth는 점점 깊어진다."
      ],
      "metadata": {
        "id": "rIvgE6VPlOR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "8LhIZc3glOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "aKdMXtiy1bT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the picture files.\n",
        "2. Decode the JPEG content to RGB grids of pixels\n",
        "3. Convert these into floating\n",
        "point tensors\n",
        "4. Resize them to a shared size (we’ll use 180\n",
        "× 180)\n",
        "5. Pack them into batches (we’ll use batches of 32 images)"
      ],
      "metadata": {
        "id": "bYb-qtb11dsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using image_dataset_from_directory to read images\n",
        "\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "metadata": {
        "id": "52IhG83OlOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gsY75MeerfoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "dkUCyI3srgi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding TensorFlow Dataset objects\n",
        "\n"
      ],
      "metadata": {
        "id": "yg8USm1Lc9Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow\n",
        "makes available the tf.data API to create efficient input pipelines\n",
        "\n",
        "The Dataset class handles many key features that would otherwise be\n",
        "cumbersome to implement yourself in particular, asynchronous data prefetching\n",
        "\n",
        "The Dataset class also exposes a functional\n",
        "style API for modifying datasets"
      ],
      "metadata": {
        "id": "e-9vJqpqbv7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
        "# from_tensor_slices() class can be used to create a Dataset from a NumPy array"
      ],
      "metadata": {
        "id": "DXMr-J0FlOId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Yielding single samples\n",
        "\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "65M3ew9_lN64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use .batch() method to batch the data\n",
        "\n",
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "zA10laH5lNxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CciDXj1NrVFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Range of useful dataset methods"
      ],
      "metadata": {
        "id": "m2vOuSd2c7p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* .shuffle(buffer_size) : Shuffles elements within a buffer\n",
        "* .prefetch (buffer_size) : Prefetches a buffer of elements in GPU memory to achieve\n",
        "better device utilization.\n",
        "* .map(callable) : Applies an arbitrary transformation to each element of the dataset"
      ],
      "metadata": {
        "id": "2VgTh1HCdH6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "xm_k1z_vli0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4))).batch(32)\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "U573UPfirnm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다시 원래 문제로 돌아가자"
      ],
      "metadata": {
        "id": "Yc82boa8rmQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the shapes of the data and labels yielded by the Dataset\n",
        "\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "BjXDskQjliyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model using a Dataset\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "C_PO7nMpliwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying curves of loss and accuracy during training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nwu2zHgQliry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "# sample이 2000개로 너무 적어 overfitting이 나타날 것이다.\n",
        "\n",
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "nvd8QAuUlioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using data augmentation to prevent overfitting"
      ],
      "metadata": {
        "id": "6lM2julrsgev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Data augmentation**\n",
        "takes the approach of generating more training data\n",
        "from existing training samples by **augmenting the samples via a number of random transformations**\n",
        "that yield believable looking images\n",
        "\n",
        "* In\n",
        "Keras , this can be done by adding a number of data augmentation layers at\n",
        "the start of your model."
      ],
      "metadata": {
        "id": "vr8eGW4Usrkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델에 다음과 같이 data_augmentation을 삽입할 수 있다.\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "vGcKdJdilikc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomFlip**(\"horizontal\")\n",
        "is for randomly flipping half the images horizontally\n",
        "\n",
        "**RandomRotation**(0.1)\n",
        "Rotates the input images by a random value in the range [ -10%, +10%]\n",
        "\n",
        "**RandomZoom**(0.2)\n",
        "Zooms in or out of the image by a random factor in the range [ -20%, +20%]"
      ],
      "metadata": {
        "id": "yMR_X34atthR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "# We can use .take(N) to only sample N batches from the dataset. This is equivalent to inserting a break in the loop after the Nth batch\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        # apply the augmentation\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        # Display the first image in the output batch.\n",
        "        # For each of the 9 iteration, this is a different augmentation of the same image\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "# augmentation을 통해 dataset이 많아지면 overfitting을 prevent할 수 있다."
      ],
      "metadata": {
        "id": "f6vTPRaUliht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a new convnet"
      ],
      "metadata": {
        "id": "qoS0XZ4XwCN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New convnet includes Image augmentation and dropout\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) # augmentation\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x) # dropout\n",
        "# dropout을 convolution layer에 사용하는 것은 좋지 않다.\n",
        "# 일반적인 Dropout은 convolution layer에 사용하지 않는다.\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Y-t0bWKtlifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the regularized convnet\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "Gt_m708FlidS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "\n",
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "# dropout과 augmentation이 없는 것보다 결과가 훨씬 좋다."
      ],
      "metadata": {
        "id": "WzoggAk7l2rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Leveraging a pretrained model"
      ],
      "metadata": {
        "id": "h1lnd-ZgmDaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A common and highly effective approach to deep learning on small image datasets\n",
        "is to use a pretrained model\n",
        "\n",
        "* **Pretrained network** is a saved network that was previously trained on a large\n",
        "dataset\n",
        "\n",
        "* Motivations:\n",
        "\n",
        "    Lots of data, time, resources needed to train and tune a neural network from\n",
        "scratch\n",
        "\n",
        "    Cheaper, faster way of adapting a neural network by exploiting their\n",
        "generalization properties"
      ],
      "metadata": {
        "id": "UYQKQ9s2oJd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Take top performing pre-trained networks(convolutional base)\n",
        "2. If we have small amount of data\n",
        "\n",
        "    Freeze all Networks + New softmax layer for cats and dogs\n",
        "\n",
        "    Training에 New softmax layer for cats and dogs만 사용한다.\n",
        "\n",
        "3. If we have larger data\n",
        "\n",
        "    Freeze some Networks + New softmax layer for cats and dogs\n",
        "\n",
        "    Training에 top performing pre-trained networks의 일부도 사용한다."
      ],
      "metadata": {
        "id": "ZgR6QxXv0zwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* List of image classification models (all pretrained on the ImageNet dataset) that are available as part of keras : Xception\n",
        ", Inception V3, ResNet50, VGG16, VGG19, MobileNet\n",
        "\n",
        "* More available from\n",
        "tensorflow hub"
      ],
      "metadata": {
        "id": "RRVXH4d9oZ6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the VGG16 convolutional base\n",
        "\n",
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False, # classifier part는 제외하고 convolutional base만 가져온다.\n",
        "    input_shape=(180, 180, 3))"
      ],
      "metadata": {
        "id": "jYOtgIdWmCpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "wu4Dv1svl2ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast feature extraction without data augmentation"
      ],
      "metadata": {
        "id": "H-3eH5tjolui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll start by extracting features as\n",
        "NumPy arrays by calling the predict()\n",
        "method of the conv_base model on our training"
      ],
      "metadata": {
        "id": "dc1xGbMLoqyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the VGG16 features and corresponding labels\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        # vgg16 pretrained network\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
        "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels =  get_features_and_labels(test_dataset)"
      ],
      "metadata": {
        "id": "qJ0GhoWgl2kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "metadata": {
        "id": "8TwhpaNul2gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining and training the densely connected classifier\n",
        "# add last layer\n",
        "# training is very fast because we only have to deal with two dense layers\n",
        "\n",
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels),\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "b2W9MrB3l2dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 2 dense layer만 사용했음에도 불구하고 결과가 좋다."
      ],
      "metadata": {
        "id": "SDCLRzWsl2Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast feature extraction with data augmentation"
      ],
      "metadata": {
        "id": "RtZDjmmwpPia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new model that chains together: \n",
        "\n",
        "1) data augmentation\n",
        "\n",
        "2) freezing convolutional base\n",
        "\n",
        "3) a dense classifier"
      ],
      "metadata": {
        "id": "g3Efjb6f7C1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating and freezing the VGG16 convolutional base\n",
        "\n",
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False) # only get convolutional base part\n",
        "conv_base.trainable = False # conv_base는 이미 잘 훈련되어있는거라 훈련시키지 않는다."
      ],
      "metadata": {
        "id": "YpREdN28pR2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the list of trainable weights before and after freezing"
      ],
      "metadata": {
        "id": "f1ZEskbn70Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "CP0pb0ubpRzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "OfKvrDgJpRwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a data augmentation stage and a classifier to the convolutional base\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) # apply data augmentation\n",
        "x = keras.applications.vgg16.preprocess_input(x) # apply input value scaling\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "SjGbraPwpRuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "-RfzSoeGpRrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "\n",
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "# 이전보다 결과가 아주 조금 좋아졌다."
      ],
      "metadata": {
        "id": "2RPzEf-vpRm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning a pretrained model"
      ],
      "metadata": {
        "id": "Lui-8FWqpn8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine\n",
        "tuning consists of unfreezing a few of the top\n",
        "layers of a frozen model base used for feature\n",
        "extraction, and jointly training both the newly added\n",
        "part of the model"
      ],
      "metadata": {
        "id": "r_lCgAK1ptpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "last convolution block을 unfreeze하고 같이 훈련시키다."
      ],
      "metadata": {
        "id": "VQO-WhqAFB2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### step"
      ],
      "metadata": {
        "id": "6xtvv4D2FdTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Add your custom network on top of an already\n",
        "trained base network\n",
        "2. Freeze the base network\n",
        "3. Train the part you added\n",
        "4. Unfreeze some layers in the base network\n",
        "5. Jointly train both these layers and the part you added"
      ],
      "metadata": {
        "id": "5JkTahvoFm55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing all layers until the fourth from the last\n",
        "\n",
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "dRCOE-cIpRjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "              # we use smaller lr\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"fine_tuning.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "zn9q-H4Cl2Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "# Many times it will improve the results"
      ],
      "metadata": {
        "id": "NUR6PrerGOKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Convnets\n",
        "are the best type of machine learning models for\n",
        "computer vision\n",
        "2. On a small dataset, overfitting will be the main issue. Data\n",
        "augmentation is a powerful way\n",
        "3. It’s easy to reuse an existing\n",
        "convnet on a new dataset via\n",
        "transfer learning\n",
        "4. As a complement to feature extraction, you can use fine\n",
        "tuning"
      ],
      "metadata": {
        "id": "qkbR-vfuHXxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch9. Advanced deep learning for computer vision"
      ],
      "metadata": {
        "id": "Il07z1u_M8h6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Three essential computer vision tasks\n",
        "2. An image segmentation example\n",
        "3. Modern\n",
        "convnet architecture patterns\n",
        "4. Interpreting what\n",
        "convnets learn"
      ],
      "metadata": {
        "id": "5HX45s0ANOPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1. Three essential computer vision tasks"
      ],
      "metadata": {
        "id": "5j-PtbCINYxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Image classification**\n",
        ": assign one or\n",
        "more labels to an image\n",
        "2. **Image segmentation**\n",
        ": goal is to\n",
        "“segment” or “partition” an image into\n",
        "different areas, with each area usually\n",
        "representing a category\n",
        "3. **Object detection**\n",
        ": goal is to draw\n",
        "rectangles (called bounding boxes)\n",
        "around objects of interest in an image,\n",
        "and associate each rectangle with a"
      ],
      "metadata": {
        "id": "BOLZLOM4NgNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2. Image segmentation example"
      ],
      "metadata": {
        "id": "ihFWVPILN5gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image segmentation with deep learning is about using a model to assign a class\n",
        "to each pixel in an image (such as “background” and “foreground,” or “road,”\n",
        "“car,” and “sidewalk\"\n",
        "\n",
        "* **Semantic segmentation**, where each pixel is independently classified into a\n",
        "semantic category\n",
        "\n",
        "* **Instance segmentation**, which seeks not only to classify image pixels by\n",
        "category, but also to parse out individual object instances"
      ],
      "metadata": {
        "id": "bQNIyY1VN5DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oxford IIIT Pets dataset"
      ],
      "metadata": {
        "id": "JbU3KeiDO97f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contains 7,390 pictures of various breeds of cats and dogs, together with\n",
        "foreground background segmentation masks\n",
        "\n",
        "**Segmentation mask**\n",
        "is the image segmentation equivalent of a label: it’s an\n",
        "image the same size as the input image, with a single color channel where each\n",
        "integer value corresponds to the class: 1 (foreground), 2 (background), and\n",
        "3(contour)"
      ],
      "metadata": {
        "id": "PC4gL2H2PNJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz\n",
        "\n",
        "# !wget : download file from the website\n",
        "# !tar : unzip file"
      ],
      "metadata": {
        "id": "Y_AtguN4NNRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory 안에 있는 file 확인\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "xRBBlZB3QXAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory 안에 있는 file 확인\n",
        "\n",
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "K3uovbhXQxU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('images')"
      ],
      "metadata": {
        "id": "vfQ_B-pPQ40x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnms1 = os.listdir('images')\n",
        "len(fnms1)"
      ],
      "metadata": {
        "id": "ZvDv5EBnQ-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('annotations')\n",
        "# annotation : 주석"
      ],
      "metadata": {
        "id": "rWxXInomROyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat annotations/README"
      ],
      "metadata": {
        "id": "Pxk7Vcb2RYk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('annotations/trimaps/')"
      ],
      "metadata": {
        "id": "Lm5fJVsjRpQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnms2 = os.listdir('annotations/trimaps/')\n",
        "len(fnms2)\n",
        "# fnms1보다 크다 : 중복 파일이 존재한다는 의미"
      ],
      "metadata": {
        "id": "4fmGKalmRxjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [os.path.join(input_dir, fname)     # join해라\n",
        "     for fname in os.listdir(input_dir) # input_dir에 있는 fname을\n",
        "     if fname.endswith(\".jpg\")])        # fname이 .jpg로 끝나면\n",
        "\n",
        "target_paths = sorted(\n",
        "    [os.path.join(target_dir, \n",
        "                  fname)\n",
        "     for fname in os.listdir(target_dir)\n",
        "     if fname.endswith(\".png\") and not fname.startswith(\".\")]) # 중복 파일 제거"
      ],
      "metadata": {
        "id": "KJ0kc8GlPcDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img_paths[:5]"
      ],
      "metadata": {
        "id": "kHyzwMNiWub7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_paths[:5]"
      ],
      "metadata": {
        "id": "gRw77_boXPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_img_paths)"
      ],
      "metadata": {
        "id": "badJXTzFXXI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_paths)\n",
        "# 중복 파일 제거 성공"
      ],
      "metadata": {
        "id": "5ZC15vNJXaML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10번째 이미지\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(load_img(input_img_paths[9]))"
      ],
      "metadata": {
        "id": "3vxwl0PfYQJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# annotation\n",
        "\n",
        "def display_target(target_array):\n",
        "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(normalized_array[:, :, 0])\n",
        "\n",
        "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
        "display_target(img)"
      ],
      "metadata": {
        "id": "8LbXMDX9YQIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our inputs and targets into two NumPy arrays\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "img_size = (200, 200)\n",
        "# resize everything\n",
        "num_imgs = len(input_img_paths)\n",
        "# total number of samples in the data\n",
        "\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_paths)\n",
        "# seed number를 1337로 동일하게 지정해줘서 input과 target이 same order를 가지면서 shuffle 될 수 있다.\n",
        "\n",
        "def path_to_input_image(path):\n",
        "    return img_to_array(load_img(path, target_size=img_size))\n",
        "\n",
        "def path_to_target(path):\n",
        "    img = img_to_array(\n",
        "        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
        "    img = img.astype(\"uint8\") - 1\n",
        "    return img\n",
        "\n",
        "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
        "# (num_imgs,)는 7000, img_size는 위에서 resize한 대로 (200, 200), RGB라서 (3,)\n",
        "# 따라서 결론적으로 (7000, 200, 200, 3)\n",
        "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
        "# (7000, 200, 200, 1)\n",
        "# 마지막 1은 1 or 2 or 3 셋 중에 한 숫자가 들어감\n",
        "for i in range(num_imgs):\n",
        "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
        "    targets[i] = path_to_target(target_paths[i])\n",
        "\n",
        "# validation을 위한 1000개의 sample\n",
        "num_val_samples = 1000\n",
        "\n",
        "# split the data into training and validation\n",
        "train_input_imgs = input_imgs[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_input_imgs = input_imgs[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]"
      ],
      "metadata": {
        "id": "LxzrhyluYQGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_imgs.shape"
      ],
      "metadata": {
        "id": "2ABWFPX9sJsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets.shape"
      ],
      "metadata": {
        "id": "-89etlkXsMvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modeling\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,)) # (200, 200, 3)\n",
        "    x = layers.Rescaling(1./255)(inputs) # rescale\n",
        "\n",
        "    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    # maxpooling을 사용하지 않고 stride 사용\n",
        "\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-CiXU5K_YQEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(img_size=img_size, num_classes=3)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uLzlsYcvYQCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The first half\n",
        "of the model closely resembles the kind of\n",
        "convnet you’d use for image classification\n",
        "\n",
        "Encode the images into smaller feature maps that contain\n",
        "spatial information about original image\n",
        "\n",
        "Downsample\n",
        "by adding strides rather than using\n",
        "maxpooling because we care a lot about the spatial location\n",
        "of information, **maxpooling destroy location information** (stride는 spatial location information이 남아있다.)"
      ],
      "metadata": {
        "id": "_bkiuR5Hbpk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The second half\n",
        "of the model is a stack of\n",
        "Conv2DTranspose layers, inverse of the transformations\n",
        "\n",
        "Transformation going in the opposite direction of\n",
        "convolutions"
      ],
      "metadata": {
        "id": "MdwnlDPpcaOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Up sampling\n",
        "\n",
        "Motivation : Need a transformation going in the opposite direction of convolutions\n",
        "\n",
        "* Generating images involving up sampling from low resolution to high resolution\n",
        "\n",
        "* Decoding layer of a convolutional auto encoder\n",
        "\n",
        "Neural network up\n",
        "samplings: Transposed convolution, Fractionally strided\n",
        "convolution"
      ],
      "metadata": {
        "id": "B_dOuKOkqG1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transposed convolution"
      ],
      "metadata": {
        "id": "J2oiOOSlqg1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Going backward of a convolution operation such that it has the similar positional\n",
        "connectivity and forms a one to many relationship\n",
        "\n",
        "* We can express a convolution\n",
        "operation using a convolution\n",
        "matrix, which is nothing but a\n",
        "rearranged matrix\n",
        "\n",
        "* We similarly express a transposed\n",
        "convolution using a transposed\n",
        "convolution matrix, whose layout is\n",
        "a transposed shape but in which\n",
        "the actual weight values does not\n",
        "have to come from the original\n",
        "convolution matrix"
      ],
      "metadata": {
        "id": "KzwCfR2Vq5Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and fit\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "# 원 핫 인코딩을 한다면 loss에 categorical_crossentropy도 사용 가능\n",
        "# 현재는 targets이 0, 1, 2의 값을 갖기 때문에 sparse_categorical_crossentropy 사용\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_input_imgs, train_targets,\n",
        "                    epochs=50,\n",
        "                    callbacks=callbacks,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(val_input_imgs, val_targets))"
      ],
      "metadata": {
        "id": "ulrkr1DZYP_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "EV3-71XVYP-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload our best performing model according to the validation loss,\n",
        "and demonstrate how to use it to predict a segmentation mask"
      ],
      "metadata": {
        "id": "Cr2AIPUIr0l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import array_to_img\n",
        "\n",
        "model = keras.models.load_model(\"oxford_segmentation.keras\")\n",
        "\n",
        "i = 4\n",
        "test_image = val_input_imgs[i]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(array_to_img(test_image))\n",
        "\n",
        "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
        "\n",
        "def display_mask(pred):\n",
        "    mask = np.argmax(pred, axis=-1)\n",
        "    mask *= 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(mask)\n",
        "\n",
        "display_mask(mask)"
      ],
      "metadata": {
        "id": "miNvJBZnYP7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3. Modern convnet architecture patterns"
      ],
      "metadata": {
        "id": "BhaiOFQ8tHqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good model architecture is one that\n",
        "reduces the size of the search space or\n",
        "otherwise makes it easier to converge to a good point of the search space\n",
        "\n",
        "Model architecture is more an art than a science. Experienced machine learning\n",
        "engineers are able to\n",
        "intuitively cobble together high performing models on\n",
        "their first try, while beginners often struggle to create a model that trains at all\n",
        "\n",
        "You’ll develop your own\n",
        "intuition throughout this book\n",
        "\n",
        "In the following sections, we’ll review a few essential\n",
        "convnet architecture best\n",
        "practices:\n",
        "**residual connections , batch normalization , and separable convolutions**\n",
        "\n",
        "We will apply them to our cat vs. dog classification problem"
      ],
      "metadata": {
        "id": "IQo2vtxvtHYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rdsidual connections"
      ],
      "metadata": {
        "id": "sDNnYyBPzKMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "너무 많은 layer를 쌓으면 결과가 converge하지 않는 문제가 발생한다.\n",
        "\n",
        "residual connection을 통해 layer를 많이 쌓아도 문제가 발생하지 않도록 할 수 있다.\n",
        "\n",
        "The residual connection acts as an\n",
        "information shortcut around destructive or\n",
        "noisy blocks"
      ],
      "metadata": {
        "id": "qiRZI_qwfGEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual block where the number of filters changes\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "# x : (32, 32, 32)\n",
        "residual = x\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "# x : (32, 32, 64)\n",
        "residual = layers.Conv2D(64, 1)(residual)\n",
        "# 차원이 달라 계산할 수 없으므로 1X1 Conv2D layer를 이용한다.\n",
        "x = layers.add([x, residual])"
      ],
      "metadata": {
        "id": "MZRoQ84YtU8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the block includes maxpooling layer\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "# x : (32, 32, 32)\n",
        "residual = x\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
        "# x : (16, 16, 64)\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual)  # apply Conv2D of 1X1 filter.\n",
        "# (16, 16, 64)\n",
        "x = layers.add([x, residual])\n"
      ],
      "metadata": {
        "id": "Dfo0-Qv0iq3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch normalization"
      ],
      "metadata": {
        "id": "Y0SsuDqRkdNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Internal Covariate Shift : distribution change of each layer’s inputs during\n",
        "training as the parameters of the previous layers change.\n",
        "\n",
        "* Inputs to each layer are a ected by the parameters of all preceding layers so that small changes to the network parameters amplify as the network becomes deeper\n",
        "\n",
        "* This requires a lower learning rate and careful parameter initialization, which slows down training and makes it notoriously hard to train models with saturating nonlinearities."
      ],
      "metadata": {
        "id": "ZGjb8MHRkdH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BN transform can be freely added to any subset of activations to be normalized."
      ],
      "metadata": {
        "id": "G5wcG27ul3QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author generally recommend placing the previous layer’s activation after\n",
        "the batch normalization layer (although this is still a subject of debate)"
      ],
      "metadata": {
        "id": "pUWLZkV4mMkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Conv2D(32, 3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x) # activation 을 batch 이후에 두는 것을 추천"
      ],
      "metadata": {
        "id": "vrGlraAfkT66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래와 같이 작성할 수도 있지만 위를 추천\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)"
      ],
      "metadata": {
        "id": "YXDX0PPvmwH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depthwise separable convolutions"
      ],
      "metadata": {
        "id": "8rc3JGsrm98U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depthwise\n",
        "separable convolution ( Depthwise Conv + Pointwise Conv ) is used to\n",
        "build a light weight CNN (fewer parameters and multiply adds) for efficient on device\n",
        "intelligence."
      ],
      "metadata": {
        "id": "-MoVHoV0m93i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A mini Xception like model\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
        "\n",
        "for size in [32, 64, 128, 256, 512]:\n",
        "    residual = x\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "    # batch normalization을 해주는 부분\n",
        "    # 결과는 Conv2D가 조금 더 좋지만 속도는 Separable2D가 빠르다.\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "    residual = layers.Conv2D(\n",
        "    size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "FOEa6ryHnA6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JVmlVQU1IoB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4. Interpreting what convnets learn"
      ],
      "metadata": {
        "id": "V_BPzVTYIsFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing intermediate activations"
      ],
      "metadata": {
        "id": "0iRqLjV-N1qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The representations learned by\n",
        "convnets are highly\n",
        "amenable to visualization\n",
        "\n",
        "* Visualizing\n",
        "intermediate convnet outputs\n",
        "* Visualizing\n",
        "convnets filters\n",
        "* Visualizing\n",
        "heatmaps of class activation in an image"
      ],
      "metadata": {
        "id": "nRXX0YNaJBss"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-VHmvBLrWnE"
      },
      "outputs": [],
      "source": [
        "# You can use this to load the file \"convnet_from_scratch_with_augmentation.keras\"\n",
        "# you obtained in the last chapter.\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "rtdMvwauLjke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGaNgVIcrWnF"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx8qu_7IrWnF"
      },
      "source": [
        "**Preprocessing a single image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZWQ_9mnrWnG"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "img_path = keras.utils.get_file(\n",
        "    fname=\"cat.jpg\",\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
        "\n",
        "# convert image to array\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = keras.utils.load_img(\n",
        "        img_path, target_size=target_size)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "img_tensor = get_img_array(img_path, target_size=(180, 180))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rhIptDFrWnH"
      },
      "source": [
        "**Displaying the test picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOt4mUvirWnH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddgj1FdarWnI"
      },
      "source": [
        "**Instantiating a model that returns layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7koCNHurWnI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "layer_outputs = []\n",
        "layer_names = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
        "        layer_outputs.append(layer.output)\n",
        "        layer_names.append(layer.name)\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers"
      ],
      "metadata": {
        "id": "-fx7AZVANG3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_names"
      ],
      "metadata": {
        "id": "Wr6UtPLpL5vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_OcgIltrWnJ"
      },
      "source": [
        "**Using the model to compute layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcbKlCunrWnJ"
      },
      "outputs": [],
      "source": [
        "# feed images to activation model\n",
        "\n",
        "activations = activation_model.predict(img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(activations)\n",
        "\n",
        "# 9 layer가 있기 때문에 9"
      ],
      "metadata": {
        "id": "YbR9ud13Ousj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKDMWMR4rWnJ"
      },
      "outputs": [],
      "source": [
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fxi2ZkBrWnK"
      },
      "source": [
        "**Visualizing the fifth channel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAI-Jw5YrWnK"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 convnet을 거친 이미지\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# maxpooling을 거친 이미지\n",
        "plt.matshow(activations[1][0, :, :, 5], cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "LTn17y8nQKz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 번째 convnet을 거친 이미지\n",
        "# deeper convnet activations are more abstract\n",
        "plt.matshow(activations[2][0, :, :, 5], cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "CQ3vhTXTQghM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 activation\n",
        "plt.matshow(activations[8][0, :, :, 5], cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "S_xtJhMqRGek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2raE5zxErWnK"
      },
      "source": [
        "**Visualizing every channel in every intermediate activation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eujrDmP4rWnK"
      },
      "outputs": [],
      "source": [
        "# activation output을 visualize해주는 코드(생략)\n",
        "\n",
        "images_per_row = 16\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    n_features = layer_activation.shape[-1]\n",
        "    size = layer_activation.shape[1]\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros(((size + 1) * n_cols - 1,\n",
        "                             images_per_row * (size + 1) - 1))\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_index = col * images_per_row + row\n",
        "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
        "            if channel_image.sum() != 0:\n",
        "                channel_image -= channel_image.mean()\n",
        "                channel_image /= channel_image.std()\n",
        "                channel_image *= 64\n",
        "                channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
        "            display_grid[\n",
        "                col * (size + 1): (col + 1) * size + col,\n",
        "                row * (size + 1) : (row + 1) * size + row] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")\n",
        "\n",
        "    # relu를 거치기 때문에 갈수록 드랍되는 레이어가 많아진다.\n",
        "    # 위의 레이어일수록 고양이의 모습이 많이 남아있다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Things to note:\n",
        "•\n",
        "First layer acts as a collection of various edge detectors,\n",
        "activations retain almost all of the information present in\n",
        "the initial picture\n",
        "\n",
        "•\n",
        "As you go higher, the activations become increasingly\n",
        "abstract and less visually interpretable.\n",
        "\n",
        "•\n",
        "The sparsity of the activations increases with the depth of\n",
        "the layer"
      ],
      "metadata": {
        "id": "aLRmc_OeOgM4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaBHT38mrWnL"
      },
      "source": [
        "### Visualizing convnet filters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the visual pattern that each filter is meant to respond to\n",
        "\n",
        "To maximize the response of a specific filter"
      ],
      "metadata": {
        "id": "XfBJbC9kRmve"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG8Omh-HrWnL"
      },
      "source": [
        "**Instantiating the Xception convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ChG3Yb8rWnL"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmdCyH4ZrWnM"
      },
      "source": [
        "**Printing the names of all convolutional layers in Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZRMKw2OrWnM"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
        "        print(layer.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykr7x01NrWnM"
      },
      "source": [
        "**Creating a feature extractor model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIvJOBR6rWnM"
      },
      "outputs": [],
      "source": [
        "layer_name = \"block3_sepconv1\"\n",
        "layer = model.get_layer(name=layer_name)\n",
        "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaCYxLVsrWnN"
      },
      "source": [
        "**Using the feature extractor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvp1jDRLrWnN"
      },
      "outputs": [],
      "source": [
        "activation = feature_extractor(\n",
        "    keras.applications.xception.preprocess_input(img_tensor)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_tensor.shape"
      ],
      "metadata": {
        "id": "qLOplqQqhNkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation.shape"
      ],
      "metadata": {
        "id": "QZ7Sfz_XhQgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkZ0qfsbrWnN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def compute_loss(image, filter_index):\n",
        "    activation = feature_extractor(image)\n",
        "    filter_activation = activation[:, 2:-2, 2:-2, filter_index] # 테두리 제거\n",
        "    return tf.reduce_mean(filter_activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3KHL-IMrWnN"
      },
      "source": [
        "**Loss maximization via stochastic gradient ascent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_tlYWXDrWnN"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def gradient_ascent_step(image, filter_index, learning_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        loss = compute_loss(image, filter_index)\n",
        "    grads = tape.gradient(loss, image)\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    image += learning_rate * grads\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJUkpHVmrWnO"
      },
      "source": [
        "**Function to generate filter visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPYqZYlqrWnO"
      },
      "outputs": [],
      "source": [
        "img_width = 200\n",
        "img_height = 200\n",
        "\n",
        "def generate_filter_pattern(filter_index):\n",
        "    iterations = 30\n",
        "    learning_rate = 10.\n",
        "    image = tf.random.uniform(\n",
        "        minval=0.4,\n",
        "        maxval=0.6,\n",
        "        shape=(1, img_width, img_height, 3))\n",
        "    for i in range(iterations):\n",
        "        image = gradient_ascent_step(image, filter_index, learning_rate)\n",
        "    return image[0].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiW7Dki_rWnO"
      },
      "source": [
        "**Utility function to convert a tensor into a valid image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl3KJ8JSrWnO"
      },
      "outputs": [],
      "source": [
        "def deprocess_image(image):\n",
        "    image -= image.mean()\n",
        "    image /= image.std()\n",
        "    image *= 64\n",
        "    image += 128\n",
        "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
        "    image = image[25:-25, 25:-25, :]\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI1xGHahrWnO"
      },
      "outputs": [],
      "source": [
        "plt.axis(\"off\")\n",
        "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))\n",
        "\n",
        "# 뒤로 갈수록 복잡해진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LG8-24vrWnP"
      },
      "source": [
        "**Generating a grid of all filter response patterns in a layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1xOmey2rWnP"
      },
      "outputs": [],
      "source": [
        "all_images = []\n",
        "for filter_index in range(64):\n",
        "    print(f\"Processing filter {filter_index}\")\n",
        "    image = deprocess_image(\n",
        "        generate_filter_pattern(filter_index)\n",
        "    )\n",
        "    all_images.append(image)\n",
        "\n",
        "margin = 5\n",
        "n = 8\n",
        "cropped_width = img_width - 25 * 2\n",
        "cropped_height = img_height - 25 * 2\n",
        "width = n * cropped_width + (n - 1) * margin\n",
        "height = n * cropped_height + (n - 1) * margin\n",
        "stitched_filters = np.zeros((width, height, 3))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        image = all_images[i * n + j]\n",
        "        stitched_filters[\n",
        "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
        "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
        "            + cropped_height,\n",
        "            :,\n",
        "        ] = image\n",
        "\n",
        "keras.utils.save_img(\n",
        "    f\"filters_for_layer_{layer_name}.png\", stitched_filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nevojy8ZrWnP"
      },
      "source": [
        "### Visualizing heatmaps of class activation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which parts of a given image led a convnet to its final\n",
        "classification decision\n",
        "\n",
        "producing heatmaps of class activation over input images"
      ],
      "metadata": {
        "id": "hmDpqYXJoXti"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKorRbiQrWnP"
      },
      "source": [
        "**Loading the Xception network with pretrained weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMmsy33ZrWnP"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception(weights=\"imagenet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alzy3YrQrWnQ"
      },
      "source": [
        "**Preprocessing an input image for Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqwA14FQrWnQ"
      },
      "outputs": [],
      "source": [
        "# download the elephant images\n",
        "img_path = keras.utils.get_file(\n",
        "    fname=\"elephant.jpg\",\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n",
        "\n",
        "# convert image to array\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    array = keras.applications.xception.preprocess_input(array)\n",
        "    return array\n",
        "\n",
        "img_array = get_img_array(img_path, target_size=(299, 299))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " img_array.shape"
      ],
      "metadata": {
        "id": "oSB31cO9wFxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(keras.utils.load_img(img_path, target_size=(299, 299)))"
      ],
      "metadata": {
        "id": "R7vGgRs9wS3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge_lhwmOrWnQ"
      },
      "outputs": [],
      "source": [
        "# prediction\n",
        "\n",
        "preds = model.predict(img_array)\n",
        "print(keras.applications.xception.decode_predictions(preds, top=3)[0])\n",
        "\n",
        "# afican elephant일 가능성이 87%로 가장 높다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCxVSqUprWnQ"
      },
      "outputs": [],
      "source": [
        "np.argmax(preds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize which parts of the image are the most African elephant like, let’s set up the Grad CAM process"
      ],
      "metadata": {
        "id": "FEIwyT9Gpt4I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APkHbBd1rWnQ"
      },
      "source": [
        "**Setting up a model that returns the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6QQIn33rWnQ"
      },
      "outputs": [],
      "source": [
        "# Create a model that maps the input image to the activations of the last convolutional layer.\n",
        "\n",
        "last_conv_layer_name = \"block14_sepconv2_act\"\n",
        "classifier_layer_names = [\n",
        "    \"avg_pool\",\n",
        "    \"predictions\",\n",
        "]\n",
        "last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUr5PPIZrWnR"
      },
      "source": [
        "**Reapplying the classifier on top of the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_kZtWD-rWnR"
      },
      "outputs": [],
      "source": [
        "# Create a model that maps the activations of the last convolutional layer to the final class predictions.\n",
        "\n",
        "classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "x = classifier_input\n",
        "for layer_name in classifier_layer_names:\n",
        "    x = model.get_layer(layer_name)(x)\n",
        "classifier_model = keras.Model(classifier_input, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y0uP7UFrWnR"
      },
      "source": [
        "**Retrieving the gradients of the top predicted class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWskKNBPrWnR"
      },
      "outputs": [],
      "source": [
        "# Compute the gradient of the top predicted class for our input image with respect to the activations of the last convolution layer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "    tape.watch(last_conv_layer_output)\n",
        "    preds = classifier_model(last_conv_layer_output)\n",
        "    top_pred_index = tf.argmax(preds[0])\n",
        "    top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Sx5yPsrWnR"
      },
      "source": [
        "**Gradient pooling and channel-importance weighting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF2NjEJ3rWnR"
      },
      "outputs": [],
      "source": [
        "# Apply pooling and importance weighting to the gradient tensor to obtain our heatmap of class activation\n",
        "\n",
        "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()\n",
        "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "for i in range(pooled_grads.shape[-1]):\n",
        "    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "heatmap = np.mean(last_conv_layer_output, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjHkUK2IrWnS"
      },
      "source": [
        "**Heatmap post-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aBSIFHUrWnS"
      },
      "outputs": [],
      "source": [
        "# For visualization purposes, we’ll also normalize the heatmap between 0 and 1.\n",
        "\n",
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sFV9x_SrWnS"
      },
      "source": [
        "**Superimposing the heatmap on the original picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44qU7yn1rWnS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "img = keras.utils.load_img(img_path)\n",
        "img = keras.utils.img_to_array(img)\n",
        "\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "jet = cm.get_cmap(\"jet\")\n",
        "jet_colors = jet(np.arange(256))[:, :3]\n",
        "jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "superimposed_img = jet_heatmap * 0.4 + img\n",
        "superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "save_path = \"elephant_cam.jpg\"\n",
        "superimposed_img.save(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(superimposed_img)"
      ],
      "metadata": {
        "id": "pkNji_MY0jrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OptRpEXA0nIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V6TKx1XOJk_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lJVRsl8Jcak"
      },
      "source": [
        "# Ch10. Deep learning for timeseries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Different kinds of\n",
        "timeseries tasks\n",
        "\n",
        "* A temperature\n",
        "forecasting example\n",
        "\n",
        "* Understanding recurrent neural networks\n",
        "\n",
        "* Advanced use of recurrent neural networks"
      ],
      "metadata": {
        "id": "RhoMm8hZJ4Wo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjRNHLv8Jcal"
      },
      "source": [
        "## Different kinds of timeseries tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Forecasting : predicting what will happen next in a series\n",
        "* Classification\n",
        ": Assign one or more categorical labels to a\n",
        "timeseries .\n",
        "* Event detection\n",
        ": Identify the occurrence of a specific\n",
        "expected event within a continuous data stream\n",
        "* Anomaly detection\n",
        ": Detect anything unusual happening\n",
        "within a continuous datastream"
      ],
      "metadata": {
        "id": "L5PzK2PCJ_Yz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr16DEAbJcal"
      },
      "source": [
        "## A temperature-forecasting example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weather timeseries dataset recorded at the weather station\n",
        "at the Max Planck Institute for Biogeochemistry in Jena,\n",
        "Germany"
      ],
      "metadata": {
        "id": "2XQ8a_H8KVX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features:\n",
        "14 different quantities (such as temperature,\n",
        "pressure, humidity, wind direction, and so on) were recorded\n",
        "every 10 minutes over 2009~2016"
      ],
      "metadata": {
        "id": "WbQ-AtzoKTgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eZ0II2pJcam"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s53akp7tJcan"
      },
      "source": [
        "**Inspecting the data of the Jena weather dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjOsGS6hJcan"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "fname = os.path.join(\"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUk4KtyKJcao"
      },
      "source": [
        "**Parsing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7jZQLCXJcao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1-tWGJGJcap"
      },
      "source": [
        "**Plotting the temperature timeseries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-pqDxlIJcaq"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(range(len(temperature)), temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqJEmwP5Jcaq"
      },
      "source": [
        "**Plotting the first 10 days of the temperature timeseries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdb2Yls_Jcaq"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1440), temperature[:1440])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hhosjSlJcar"
      },
      "source": [
        "**Computing the number of samples we'll use for each data split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HGYzoddJcar"
      },
      "outputs": [],
      "source": [
        "num_train_samples = int(0.5 * len(raw_data))\n",
        "num_val_samples = int(0.25 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "print(\"num_train_samples:\", num_train_samples)\n",
        "print(\"num_val_samples:\", num_val_samples)\n",
        "print(\"num_test_samples:\", num_test_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlMslT-EJcar"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN6IvieaJcar"
      },
      "source": [
        "**Normalizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDbNjlGSJcas"
      },
      "outputs": [],
      "source": [
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16JSmEWKJcas"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "int_sequence = np.arange(10)\n",
        "dummy_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    data=int_sequence[:-3],\n",
        "    targets=int_sequence[3:],\n",
        "    sequence_length=3,\n",
        "    batch_size=2,\n",
        ")\n",
        "\n",
        "for inputs, targets in dummy_dataset:\n",
        "    for i in range(inputs.shape[0]):\n",
        "        print([int(x) for x in inputs[i]], int(targets[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqMuzZvqJcas"
      },
      "source": [
        "**Instantiating datasets for training, validation, and testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5lojHIyJcat"
      },
      "outputs": [],
      "source": [
        "sampling_rate = 6\n",
        "sequence_length = 120\n",
        "delay = sampling_rate * (sequence_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples)\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples)\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut0Q4Ba7Jcat"
      },
      "source": [
        "**Inspecting the output of one of our datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CpbAhTJcat"
      },
      "outputs": [],
      "source": [
        "for samples, targets in train_dataset:\n",
        "    print(\"samples shape:\", samples.shape)\n",
        "    print(\"targets shape:\", targets.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXxbCtaKJcau"
      },
      "source": [
        "### A common-sense, non-machine-learning baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEPjRu4GJcau"
      },
      "source": [
        "**Computing the common-sense baseline MAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcuSPgmmJcau"
      },
      "outputs": [],
      "source": [
        "def evaluate_naive_method(dataset):\n",
        "    total_abs_err = 0.\n",
        "    samples_seen = 0\n",
        "    for samples, targets in dataset:\n",
        "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
        "        total_abs_err += np.sum(np.abs(preds - targets))\n",
        "        samples_seen += samples.shape[0]\n",
        "    return total_abs_err / samples_seen\n",
        "\n",
        "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
        "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOI2Z02bJcau"
      },
      "source": [
        "### Let's try a basic machine-learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTE14ueJcau"
      },
      "source": [
        "**Training and evaluating a densely connected model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDuZx2-ZJcav"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_dense.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_dense.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcNyCDpAJcav"
      },
      "source": [
        "**Plotting results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GINhXO67Jcav"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss = history.history[\"mae\"]\n",
        "val_loss = history.history[\"val_mae\"]\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
        "plt.title(\"Training and validation MAE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jcZTrD-Jcav"
      },
      "source": [
        "### Let's try a 1D convolutional model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7W-fxPiJcav"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Conv1D(8, 24, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 12, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 6, activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_conv.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_conv.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLW22r0QJcaw"
      },
      "source": [
        "### A first recurrent baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KxoSNfPJcaw"
      },
      "source": [
        "**A simple LSTM-based model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqZ4o-v3Jcaw"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(16)(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_lstm.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jc2W-whJcaw"
      },
      "source": [
        "## Understanding recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwbSqANJJcaw"
      },
      "source": [
        "**NumPy implementation of a simple RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7pAIWnGJcaw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "timesteps = 100\n",
        "input_features = 32\n",
        "output_features = 64\n",
        "inputs = np.random.random((timesteps, input_features))\n",
        "state_t = np.zeros((output_features,))\n",
        "W = np.random.random((output_features, input_features))\n",
        "U = np.random.random((output_features, output_features))\n",
        "b = np.random.random((output_features,))\n",
        "successive_outputs = []\n",
        "for input_t in inputs:\n",
        "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
        "    successive_outputs.append(output_t)\n",
        "    state_t = output_t\n",
        "final_output_sequence = np.stack(successive_outputs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wIXDmrBJcax"
      },
      "source": [
        "### A recurrent layer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8td5EHWjJcax"
      },
      "source": [
        "**An RNN layer that can process sequences of any length**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfSjubK9Jcax"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "inputs = keras.Input(shape=(None, num_features))\n",
        "outputs = layers.SimpleRNN(16)(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XcVfX8rJcax"
      },
      "source": [
        "**An RNN layer that returns only its last output step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpVC2hG9Jcax"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences=False)(inputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7GOhrZfJcax"
      },
      "source": [
        "**An RNN layer that returns its full output sequence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66U6SB4XJcay"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences=True)(inputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmLBkpUCJcay"
      },
      "source": [
        "**Stacking RNN layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCA5Fm23Jcay"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "x = layers.SimpleRNN(16, return_sequences=True)(inputs)\n",
        "x = layers.SimpleRNN(16, return_sequences=True)(x)\n",
        "outputs = layers.SimpleRNN(16)(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90OdDs-8Jcay"
      },
      "source": [
        "## Advanced use of recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66R30_IGJcay"
      },
      "source": [
        "### Using recurrent dropout to fight overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZujDoInJca0"
      },
      "source": [
        "**Training and evaluating a dropout-regularized LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjAGW8bhJca1"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt5ux59_Jca1"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, num_features))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.2, unroll=True)(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w37Zm5AJca1"
      },
      "source": [
        "### Stacking recurrent layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DziIAKQEJca1"
      },
      "source": [
        "**Training and evaluating a dropout-regularized, stacked GRU model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZhOOnjYJca1"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P4hResPJca1"
      },
      "source": [
        "### Using bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWSXI21SJca2"
      },
      "source": [
        "**Training and evaluating a bidirectional LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PKNyfSOJca2"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Bidirectional(layers.LSTM(16))(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_YkjZGHJca2"
      },
      "source": [
        "### Going even further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGyuUdCbJca2"
      },
      "source": [
        "## Summary"
      ]
    }
  ]
}