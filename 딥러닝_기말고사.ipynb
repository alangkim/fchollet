{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝_기말고사.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNIMQW98abq1EAXFbi7bfyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alangkim/fchollet/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch8. Introduction to deep learning for computer vision"
      ],
      "metadata": {
        "id": "95Z4hsdS-38b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction to\n",
        "convnets\n",
        "\n",
        "2. Training a\n",
        "convnet from scratch on a small dataset\n",
        "\n",
        "3. Leveraging a\n",
        "pretrained model"
      ],
      "metadata": {
        "id": "AwpvaKrb_AAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction to convnets"
      ],
      "metadata": {
        "id": "McKuibZi_Mfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stack of Conv2D and MaxPooling2D layers"
      ],
      "metadata": {
        "id": "qRT-9fvw_VmY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk2kADnJoxUw"
      },
      "outputs": [],
      "source": [
        "# Instantiating a small convnet\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(28, 28, 1))                                     # MNIST dataset을 이용하기 위해 28*28 사용\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)     # Conv2D\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)                                     # MaxPooling2D\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)                                                     # Flatten all the information\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)                         # connect Dense layer\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)                         # making model by functional API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZZyYWIxoxUw"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOxV0KpuoxUw"
      },
      "outputs": [],
      "source": [
        "# Training the convnet on MNIST images\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)) # CNN을 이용하기 위해서 channel dimension은 필수적이다.\n",
        "# Convnet is running on the original shape of the image.\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\", # multi class classification\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the convnet\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "z-wybeXvAIJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The convolution operation"
      ],
      "metadata": {
        "id": "dtZv-RvYrmcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 'Dense layers' learn 'global patterns' in their input feature space whereas 'convolution layers' learn 'local patterns'\n",
        "\n",
        "* The patterns they learn are\n",
        "translation invariant\n",
        "\n",
        "* They can learn spatial hierarchies of patterns\n",
        "\n",
        "* Convolution preserves the spatial relationship between pixels by learning image\n",
        "features using small squares (depending on the filter size) of input data\n",
        "\n",
        "* Convolution: multiplying elementwise by filter and summing the multiplication\n",
        "outputs\n",
        "\n",
        "* Ex) a 3x3 kernel or 3x3x1 filter acts on a 5x6 input image with stride 1 and outputs\n",
        "a 3x4 feature map.\n",
        "\n",
        "* In fully connected sense, we need unshared 30(=5x6)x12(=3x4) weights (input size x output size)\n",
        "\n",
        "* 9 vs 360. So using convolution filter is far more efficient."
      ],
      "metadata": {
        "id": "3ouJ_NL9rrh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution on MxNx3 image with 3x3x3 filter producing 1 feature map by taking dot products between the filter and 3x3x3 piecies of the image.\n",
        "\n",
        "Depth part is decided based on the input feature map."
      ],
      "metadata": {
        "id": "7hZiPhQpaxi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why convolution?"
      ],
      "metadata": {
        "id": "DEi2OSthb5ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fully Connected -> 1000x1000 images, 10000 hidden nodes, 10^10 parameters\n",
        "* Convolution     -> 1000x1000 images, 10x10 filter size, 100 filters, 10^4 parameters\n",
        "\n",
        "* If you are dealing with image dataset, it's highly recommend to use convolution layers in modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "my-gSQFccLmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How convolution filter works?"
      ],
      "metadata": {
        "id": "r7Tjwn0OeVh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different values of the filter matrix produce different\n",
        "feature maps for the same input image.\n",
        "\n",
        "CNN learns the values of filters during training\n",
        "\n",
        "The more filters, the more features are extracted"
      ],
      "metadata": {
        "id": "jFMAcI2_eHt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature map"
      ],
      "metadata": {
        "id": "TtSwIzOsXFCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4 parameters of feature map\n",
        "\n",
        "1. filter size\n",
        "2. depth\n",
        "3. stride\n",
        "4. zero-padding"
      ],
      "metadata": {
        "id": "zL5fMh7yXVBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The max pooling operation"
      ],
      "metadata": {
        "id": "eXn7q_J9XsfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Role\n",
        "of max pooling: to aggressively downsample feature maps\n",
        "\n",
        "Transformed via a hardcoded max\n",
        "tensor operation\n",
        "\n",
        "We need the features from the last\n",
        "convolution layer to contain\n",
        "information about the totality of the\n",
        "input\n",
        "\n",
        "The final feature map has 22\n",
        "× 22 ×\n",
        "128 = 61,952 total coefficients per\n",
        "sample\n",
        "\n",
        "This is far too large for such a\n",
        "small model and would result in\n",
        "intense overfitting"
      ],
      "metadata": {
        "id": "gVJW72urXwjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max-pooling이 없는 경우\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "j3p6KvfpAIFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool.summary()\n",
        "# 모델의 크기에 비해 parameters가 너무 많다."
      ],
      "metadata": {
        "id": "vcWDvOrWAIC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max-pooling은 없지만 stride를 2로 지정한 경우\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")(inputs) # stride = 2 로 지정.\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "QoaoW4dZAH8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool.summary()\n",
        "# parameters가 많이 줄어들었으나 max-pooling의 결과가 더 좋다.\n",
        "# 일반적으로 classification에서는 stride보다 max-pooling을 자주 사용한다.\n",
        "# 경험적으로 대부분 average-pooling보다 max-poolng이 좋다."
      ],
      "metadata": {
        "id": "NHP6j9KlAHr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training a convnet from scratch on a small dataset"
      ],
      "metadata": {
        "id": "gyYyyqIkhznz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading a\n",
        "Kaggle dataset in Google Colaboratory\n",
        "\n",
        "Access to the API is restricted to\n",
        "Kaggle users, you need to authenticate yourself.\n",
        "\n",
        "The\n",
        "kaggle package will look for your login credentials in a JSON file located at\n",
        "kaggle kaggle.json\n",
        "\n",
        "First, you need to create a\n",
        "Kaggle API key and download it to your local machine\n",
        "Login\n",
        "--> My Account --> Account settings --> API\n",
        "Click the Create New API Token\n",
        "button\n",
        "\n",
        "\n",
        "Second, go to your\n",
        "Colab notebook, and upload the API’s key JSON file to your\n",
        "Colab session by running the following code in a notebook cell:"
      ],
      "metadata": {
        "id": "PHjhFVBMh78x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러오기"
      ],
      "metadata": {
        "id": "LXeOrQMAu8M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "7yLglkdUh2AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "rpLHAqo7ioCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "metadata": {
        "id": "URLInUr9iyPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "Aoz3fYDht2G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq dogs-vs-cats.zip"
      ],
      "metadata": {
        "id": "WPYiVZdhuQeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "o4SpUBbXuhnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq train.zip"
      ],
      "metadata": {
        "id": "P7rI26zci60T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "W3c9kqftgHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('train')"
      ],
      "metadata": {
        "id": "TYgR2-MOurIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copying images to training, validation, and test directories"
      ],
      "metadata": {
        "id": "8f_Gm9WJvF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "복잡하게 나열되어있는 data를 train, validation, test로 나누고 각각 1000개, 500개, 1000개의 data를 넣는 전처리"
      ],
      "metadata": {
        "id": "biCIyvK-vQ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "# original dataset이 풀려있는 directory\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "# smaller dataset을 저장할 directory\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        # 새로운 directory 만들기 ex) cats_vs_dogs_small/train/dog\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        # 파일 이름 만들기\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "            # src : source, dst : destination\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "# 처음 1000개로 train set을 만듦\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "# 그 다음 500개로 validation set을 만듦\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)\n",
        "# 그 다음 1000개로 test set을 만듦"
      ],
      "metadata": {
        "id": "eqRdeLOwi9um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(new_base_dir)"
      ],
      "metadata": {
        "id": "7HQHyI4WgYm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 코드와 동일\n",
        "os.listdir('cats_vs_dogs_small')"
      ],
      "metadata": {
        "id": "wEcojIOcg5-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('cats_vs_dogs_small/test')"
      ],
      "metadata": {
        "id": "ukgcOCRGgmC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('cats_vs_dogs_small/test/dog')\n",
        "# 1500~2500 index를 가진 dog 파일이 들어가있음"
      ],
      "metadata": {
        "id": "V4YvE_GNgxoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "# 180x180 size를 가진 RGB image\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "# rescale\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "# binary classification이라 activation은 sigmoid\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "JL_OwU5yjMLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "# height, width는 점점 작아지고 depth는 점점 깊어진다."
      ],
      "metadata": {
        "id": "rIvgE6VPlOR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "8LhIZc3glOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "aKdMXtiy1bT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the picture files.\n",
        "2. Decode the JPEG content to RGB grids of pixels\n",
        "3. Convert these into floating\n",
        "point tensors\n",
        "4. Resize them to a shared size (we’ll use 180\n",
        "× 180)\n",
        "5. Pack them into batches (we’ll use batches of 32 images)"
      ],
      "metadata": {
        "id": "bYb-qtb11dsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using image_dataset_from_directory to read images\n",
        "\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "metadata": {
        "id": "52IhG83OlOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gsY75MeerfoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "dkUCyI3srgi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding TensorFlow Dataset objects\n",
        "\n"
      ],
      "metadata": {
        "id": "yg8USm1Lc9Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow\n",
        "makes available the tf.data API to create efficient input pipelines\n",
        "\n",
        "The Dataset class handles many key features that would otherwise be\n",
        "cumbersome to implement yourself in particular, asynchronous data prefetching\n",
        "\n",
        "The Dataset class also exposes a functional\n",
        "style API for modifying datasets"
      ],
      "metadata": {
        "id": "e-9vJqpqbv7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
        "# from_tensor_slices() class can be used to create a Dataset from a NumPy array"
      ],
      "metadata": {
        "id": "DXMr-J0FlOId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Yielding single samples\n",
        "\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "65M3ew9_lN64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use .batch() method to batch the data\n",
        "\n",
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "zA10laH5lNxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CciDXj1NrVFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Range of useful dataset methods"
      ],
      "metadata": {
        "id": "m2vOuSd2c7p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* .shuffle(buffer_size) : Shuffles elements within a buffer\n",
        "* .prefetch (buffer_size) : Prefetches a buffer of elements in GPU memory to achieve\n",
        "better device utilization.\n",
        "* .map(callable) : Applies an arbitrary transformation to each element of the dataset"
      ],
      "metadata": {
        "id": "2VgTh1HCdH6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "xm_k1z_vli0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4))).batch(32)\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "U573UPfirnm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다시 원래 문제로 돌아가자"
      ],
      "metadata": {
        "id": "Yc82boa8rmQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the shapes of the data and labels yielded by the Dataset\n",
        "\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "BjXDskQjliyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model using a Dataset\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "C_PO7nMpliwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying curves of loss and accuracy during training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nwu2zHgQliry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "# sample이 2000개로 너무 적어 overfitting이 나타날 것이다.\n",
        "\n",
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "nvd8QAuUlioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using data augmentation to prevent overfitting"
      ],
      "metadata": {
        "id": "6lM2julrsgev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Data augmentation**\n",
        "takes the approach of generating more training data\n",
        "from existing training samples by **augmenting the samples via a number of random transformations**\n",
        "that yield believable looking images\n",
        "\n",
        "* In\n",
        "Keras , this can be done by adding a number of data augmentation layers at\n",
        "the start of your model."
      ],
      "metadata": {
        "id": "vr8eGW4Usrkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델에 다음과 같이 data_augmentation을 삽입할 수 있다.\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "vGcKdJdilikc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomFlip**(\"horizontal\")\n",
        "is for randomly flipping half the images horizontally\n",
        "\n",
        "**RandomRotation**(0.1)\n",
        "Rotates the input images by a random value in the range [ -10%, +10%]\n",
        "\n",
        "**RandomZoom**(0.2)\n",
        "Zooms in or out of the image by a random factor in the range [ -20%, +20%]"
      ],
      "metadata": {
        "id": "yMR_X34atthR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "# We can use .take(N) to only sample N batches from the dataset. This is equivalent to inserting a break in the loop after the Nth batch\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        # apply the augmentation\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        # Display the first image in the output batch.\n",
        "        # For each of the 9 iteration, this is a different augmentation of the same image\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "# augmentation을 통해 dataset이 많아지면 overfitting을 prevent할 수 있다."
      ],
      "metadata": {
        "id": "f6vTPRaUliht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a new convnet"
      ],
      "metadata": {
        "id": "qoS0XZ4XwCN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New convnet includes Image augmentation and dropout\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) # augmentation\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x) # dropout\n",
        "# dropout을 convolution layer에 사용하는 것은 좋지 않다.\n",
        "# 일반적인 Dropout은 convolution layer에 사용하지 않는다.\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Y-t0bWKtlifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the regularized convnet\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "Gt_m708FlidS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "\n",
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "# dropout과 augmentation이 없는 것보다 결과가 훨씬 좋다."
      ],
      "metadata": {
        "id": "WzoggAk7l2rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Leveraging a pretrained model"
      ],
      "metadata": {
        "id": "h1lnd-ZgmDaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A common and highly effective approach to deep learning on small image datasets\n",
        "is to use a pretrained model\n",
        "\n",
        "* **Pretrained network** is a saved network that was previously trained on a large\n",
        "dataset\n",
        "\n",
        "* Motivations:\n",
        "\n",
        "    Lots of data, time, resources needed to train and tune a neural network from\n",
        "scratch\n",
        "\n",
        "    Cheaper, faster way of adapting a neural network by exploiting their\n",
        "generalization properties"
      ],
      "metadata": {
        "id": "UYQKQ9s2oJd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Take top performing pre-trained networks(convolutional base)\n",
        "2. If we have small amount of data\n",
        "\n",
        "    Freeze all Networks + New softmax layer for cats and dogs\n",
        "\n",
        "    Training에 New softmax layer for cats and dogs만 사용한다.\n",
        "\n",
        "3. If we have larger data\n",
        "\n",
        "    Freeze some Networks + New softmax layer for cats and dogs\n",
        "\n",
        "    Training에 top performing pre-trained networks의 일부도 사용한다."
      ],
      "metadata": {
        "id": "ZgR6QxXv0zwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* List of image classification models (all pretrained on the ImageNet dataset) that are available as part of keras : Xception\n",
        ", Inception V3, ResNet50, VGG16, VGG19, MobileNet\n",
        "\n",
        "* More available from\n",
        "tensorflow hub"
      ],
      "metadata": {
        "id": "RRVXH4d9oZ6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the VGG16 convolutional base\n",
        "\n",
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False, # classifier part는 제외하고 convolutional base만 가져온다.\n",
        "    input_shape=(180, 180, 3))"
      ],
      "metadata": {
        "id": "jYOtgIdWmCpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "wu4Dv1svl2ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast feature extraction without data augmentation"
      ],
      "metadata": {
        "id": "H-3eH5tjolui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll start by extracting features as\n",
        "NumPy arrays by calling the predict()\n",
        "method of the conv_base model on our training"
      ],
      "metadata": {
        "id": "dc1xGbMLoqyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the VGG16 features and corresponding labels\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        # vgg16 pretrained network\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
        "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels =  get_features_and_labels(test_dataset)"
      ],
      "metadata": {
        "id": "qJ0GhoWgl2kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "metadata": {
        "id": "8TwhpaNul2gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining and training the densely connected classifier\n",
        "# add last layer\n",
        "# training is very fast because we only have to deal with two dense layers\n",
        "\n",
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels),\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "b2W9MrB3l2dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 2 dense layer만 사용했음에도 불구하고 결과가 좋다."
      ],
      "metadata": {
        "id": "SDCLRzWsl2Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast feature extraction with data augmentation"
      ],
      "metadata": {
        "id": "RtZDjmmwpPia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new model that chains together: \n",
        "\n",
        "1) data augmentation\n",
        "\n",
        "2) freezing convolutional base\n",
        "\n",
        "3) a dense classifier"
      ],
      "metadata": {
        "id": "g3Efjb6f7C1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating and freezing the VGG16 convolutional base\n",
        "\n",
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False) # only get convolutional base part\n",
        "conv_base.trainable = False # conv_base는 이미 잘 훈련되어있는거라 훈련시키지 않는다."
      ],
      "metadata": {
        "id": "YpREdN28pR2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the list of trainable weights before and after freezing"
      ],
      "metadata": {
        "id": "f1ZEskbn70Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "CP0pb0ubpRzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "OfKvrDgJpRwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a data augmentation stage and a classifier to the convolutional base\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) # apply data augmentation\n",
        "x = keras.applications.vgg16.preprocess_input(x) # apply input value scaling\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "SjGbraPwpRuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "-RfzSoeGpRrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "\n",
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "# 이전보다 결과가 아주 조금 좋아졌다."
      ],
      "metadata": {
        "id": "2RPzEf-vpRm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning a pretrained model"
      ],
      "metadata": {
        "id": "Lui-8FWqpn8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine\n",
        "tuning consists of unfreezing a few of the top\n",
        "layers of a frozen model base used for feature\n",
        "extraction, and jointly training both the newly added\n",
        "part of the model"
      ],
      "metadata": {
        "id": "r_lCgAK1ptpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "last convolution block을 unfreeze하고 같이 훈련시키다."
      ],
      "metadata": {
        "id": "VQO-WhqAFB2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### step"
      ],
      "metadata": {
        "id": "6xtvv4D2FdTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Add your custom network on top of an already\n",
        "trained base network\n",
        "2. Freeze the base network\n",
        "3. Train the part you added\n",
        "4. Unfreeze some layers in the base network\n",
        "5. Jointly train both these layers and the part you added"
      ],
      "metadata": {
        "id": "5JkTahvoFm55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing all layers until the fourth from the last\n",
        "\n",
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "dRCOE-cIpRjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "              # we use smaller lr\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"fine_tuning.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "zn9q-H4Cl2Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "# Many times it will improve the results"
      ],
      "metadata": {
        "id": "NUR6PrerGOKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Convnets\n",
        "are the best type of machine learning models for\n",
        "computer vision\n",
        "2. On a small dataset, overfitting will be the main issue. Data\n",
        "augmentation is a powerful way\n",
        "3. It’s easy to reuse an existing\n",
        "convnet on a new dataset via\n",
        "transfer learning\n",
        "4. As a complement to feature extraction, you can use fine\n",
        "tuning"
      ],
      "metadata": {
        "id": "qkbR-vfuHXxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch9. Advanced deep learning for computer vision"
      ],
      "metadata": {
        "id": "Il07z1u_M8h6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Three essential computer vision tasks\n",
        "2. An image segmentation example\n",
        "3. Modern\n",
        "convnet architecture patterns\n",
        "4. Interpreting what\n",
        "convnets learn"
      ],
      "metadata": {
        "id": "5HX45s0ANOPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1. Three essential computer vision tasks"
      ],
      "metadata": {
        "id": "5j-PtbCINYxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Image classification**\n",
        ": assign one or\n",
        "more labels to an image\n",
        "2. **Image segmentation**\n",
        ": goal is to\n",
        "“segment” or “partition” an image into\n",
        "different areas, with each area usually\n",
        "representing a category\n",
        "3. **Object detection**\n",
        ": goal is to draw\n",
        "rectangles (called bounding boxes)\n",
        "around objects of interest in an image,\n",
        "and associate each rectangle with a"
      ],
      "metadata": {
        "id": "BOLZLOM4NgNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2. Image segmentation example"
      ],
      "metadata": {
        "id": "ihFWVPILN5gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image segmentation with deep learning is about using a model to assign a class\n",
        "to each pixel in an image (such as “background” and “foreground,” or “road,”\n",
        "“car,” and “sidewalk\"\n",
        "\n",
        "* **Semantic segmentation**, where each pixel is independently classified into a\n",
        "semantic category\n",
        "\n",
        "* **Instance segmentation**, which seeks not only to classify image pixels by\n",
        "category, but also to parse out individual object instances"
      ],
      "metadata": {
        "id": "bQNIyY1VN5DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oxford IIIT Pets dataset"
      ],
      "metadata": {
        "id": "JbU3KeiDO97f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contains 7,390 pictures of various breeds of cats and dogs, together with\n",
        "foreground background segmentation masks\n",
        "\n",
        "**Segmentation mask**\n",
        "is the image segmentation equivalent of a label: it’s an\n",
        "image the same size as the input image, with a single color channel where each\n",
        "integer value corresponds to the class: 1 (foreground), 2 (background), and\n",
        "3(contour)"
      ],
      "metadata": {
        "id": "PC4gL2H2PNJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz\n",
        "\n",
        "# !wget : download file from the website\n",
        "# !tar : unzip file"
      ],
      "metadata": {
        "id": "Y_AtguN4NNRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory 안에 있는 file 확인\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "xRBBlZB3QXAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory 안에 있는 file 확인\n",
        "\n",
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "K3uovbhXQxU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('images')"
      ],
      "metadata": {
        "id": "vfQ_B-pPQ40x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnms1 = os.listdir('images')\n",
        "len(fnms1)"
      ],
      "metadata": {
        "id": "ZvDv5EBnQ-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('annotations')\n",
        "# annotation : 주석"
      ],
      "metadata": {
        "id": "rWxXInomROyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat annotations/README"
      ],
      "metadata": {
        "id": "Pxk7Vcb2RYk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('annotations/trimaps/')"
      ],
      "metadata": {
        "id": "Lm5fJVsjRpQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnms2 = os.listdir('annotations/trimaps/')\n",
        "len(fnms2)\n",
        "# fnms1보다 크다 : 중복 파일이 존재한다는 의미"
      ],
      "metadata": {
        "id": "4fmGKalmRxjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [os.path.join(input_dir, fname)     # join해라\n",
        "     for fname in os.listdir(input_dir) # input_dir에 있는 fname을\n",
        "     if fname.endswith(\".jpg\")])        # fname이 .jpg로 끝나면\n",
        "\n",
        "target_paths = sorted(\n",
        "    [os.path.join(target_dir, \n",
        "                  fname)\n",
        "     for fname in os.listdir(target_dir)\n",
        "     if fname.endswith(\".png\") and not fname.startswith(\".\")]) # 중복 파일 제거"
      ],
      "metadata": {
        "id": "KJ0kc8GlPcDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img_paths[:5]"
      ],
      "metadata": {
        "id": "kHyzwMNiWub7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_paths[:5]"
      ],
      "metadata": {
        "id": "gRw77_boXPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_img_paths)"
      ],
      "metadata": {
        "id": "badJXTzFXXI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_paths)\n",
        "# 중복 파일 제거 성공"
      ],
      "metadata": {
        "id": "5ZC15vNJXaML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10번째 이미지\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(load_img(input_img_paths[9]))"
      ],
      "metadata": {
        "id": "3vxwl0PfYQJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# annotation\n",
        "\n",
        "def display_target(target_array):\n",
        "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(normalized_array[:, :, 0])\n",
        "\n",
        "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
        "display_target(img)"
      ],
      "metadata": {
        "id": "8LbXMDX9YQIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our inputs and targets into two NumPy arrays\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "img_size = (200, 200)\n",
        "# resize everything\n",
        "num_imgs = len(input_img_paths)\n",
        "# total number of samples in the data\n",
        "\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_paths)\n",
        "# seed number를 1337로 동일하게 지정해줘서 input과 target이 same order를 가지면서 shuffle 될 수 있다.\n",
        "\n",
        "def path_to_input_image(path):\n",
        "    return img_to_array(load_img(path, target_size=img_size))\n",
        "\n",
        "def path_to_target(path):\n",
        "    img = img_to_array(\n",
        "        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
        "    img = img.astype(\"uint8\") - 1\n",
        "    return img\n",
        "\n",
        "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
        "# (num_imgs,)는 7000, img_size는 위에서 resize한 대로 (200, 200), RGB라서 (3,)\n",
        "# 따라서 결론적으로 (7000, 200, 200, 3)\n",
        "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
        "# (7000, 200, 200, 1)\n",
        "# 마지막 1은 1 or 2 or 3 셋 중에 한 숫자가 들어감\n",
        "for i in range(num_imgs):\n",
        "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
        "    targets[i] = path_to_target(target_paths[i])\n",
        "\n",
        "# validation을 위한 1000개의 sample\n",
        "num_val_samples = 1000\n",
        "\n",
        "# split the data into training and validation\n",
        "train_input_imgs = input_imgs[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_input_imgs = input_imgs[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]"
      ],
      "metadata": {
        "id": "LxzrhyluYQGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_imgs.shape"
      ],
      "metadata": {
        "id": "2ABWFPX9sJsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets.shape"
      ],
      "metadata": {
        "id": "-89etlkXsMvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modeling\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,)) # (200, 200, 3)\n",
        "    x = layers.Rescaling(1./255)(inputs) # rescale\n",
        "\n",
        "    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    # maxpooling을 사용하지 않고 stride 사용\n",
        "\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-CiXU5K_YQEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(img_size=img_size, num_classes=3)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uLzlsYcvYQCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The first half\n",
        "of the model closely resembles the kind of\n",
        "convnet you’d use for image classification\n",
        "\n",
        "Encode the images into smaller feature maps that contain\n",
        "spatial information about original image\n",
        "\n",
        "Downsample\n",
        "by adding strides rather than using\n",
        "maxpooling because we care a lot about the spatial location\n",
        "of information, **maxpooling destroy location information** (stride는 spatial location information이 남아있다.)"
      ],
      "metadata": {
        "id": "_bkiuR5Hbpk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The second half\n",
        "of the model is a stack of\n",
        "Conv2DTranspose layers, inverse of the transformations\n",
        "\n",
        "Transformation going in the opposite direction of\n",
        "convolutions"
      ],
      "metadata": {
        "id": "MdwnlDPpcaOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Up sampling\n",
        "\n",
        "Motivation : Need a transformation going in the opposite direction of convolutions\n",
        "\n",
        "* Generating images involving up sampling from low resolution to high resolution\n",
        "\n",
        "* Decoding layer of a convolutional auto encoder\n",
        "\n",
        "Neural network up\n",
        "samplings: Transposed convolution, Fractionally strided\n",
        "convolution"
      ],
      "metadata": {
        "id": "B_dOuKOkqG1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transposed convolution"
      ],
      "metadata": {
        "id": "J2oiOOSlqg1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Going backward of a convolution operation such that it has the similar positional\n",
        "connectivity and forms a one to many relationship\n",
        "\n",
        "* We can express a convolution\n",
        "operation using a convolution\n",
        "matrix, which is nothing but a\n",
        "rearranged matrix\n",
        "\n",
        "* We similarly express a transposed\n",
        "convolution using a transposed\n",
        "convolution matrix, whose layout is\n",
        "a transposed shape but in which\n",
        "the actual weight values does not\n",
        "have to come from the original\n",
        "convolution matrix"
      ],
      "metadata": {
        "id": "KzwCfR2Vq5Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and fit\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "# 원 핫 인코딩을 한다면 loss에 categorical_crossentropy도 사용 가능\n",
        "# 현재는 targets이 0, 1, 2의 값을 갖기 때문에 sparse_categorical_crossentropy 사용\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_input_imgs, train_targets,\n",
        "                    epochs=50,\n",
        "                    callbacks=callbacks,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(val_input_imgs, val_targets))"
      ],
      "metadata": {
        "id": "ulrkr1DZYP_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "EV3-71XVYP-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload our best performing model according to the validation loss,\n",
        "and demonstrate how to use it to predict a segmentation mask"
      ],
      "metadata": {
        "id": "Cr2AIPUIr0l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import array_to_img\n",
        "\n",
        "model = keras.models.load_model(\"oxford_segmentation.keras\")\n",
        "\n",
        "i = 4\n",
        "test_image = val_input_imgs[i]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(array_to_img(test_image))\n",
        "\n",
        "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
        "\n",
        "def display_mask(pred):\n",
        "    mask = np.argmax(pred, axis=-1)\n",
        "    mask *= 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(mask)\n",
        "\n",
        "display_mask(mask)"
      ],
      "metadata": {
        "id": "miNvJBZnYP7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3. Modern convnet architecture patterns"
      ],
      "metadata": {
        "id": "BhaiOFQ8tHqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good model architecture is one that\n",
        "reduces the size of the search space or\n",
        "otherwise makes it easier to converge to a good point of the search space\n",
        "\n",
        "Model architecture is more an art than a science. Experienced machine learning\n",
        "engineers are able to\n",
        "intuitively cobble together high performing models on\n",
        "their first try, while beginners often struggle to create a model that trains at all\n",
        "\n",
        "You’ll develop your own\n",
        "intuition throughout this book\n",
        "\n",
        "In the following sections, we’ll review a few essential\n",
        "convnet architecture best\n",
        "practices:\n",
        "**residual connections , batch normalization , and separable convolutions**\n",
        "\n",
        "We will apply them to our cat vs. dog classification problem"
      ],
      "metadata": {
        "id": "IQo2vtxvtHYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rdsidual connections"
      ],
      "metadata": {
        "id": "sDNnYyBPzKMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "너무 많은 layer를 쌓으면 결과가 converge하지 않는 문제가 발생한다.\n",
        "\n",
        "residual connection을 통해 layer를 많이 쌓아도 문제가 발생하지 않도록 할 수 있다.\n",
        "\n",
        "The residual connection acts as an\n",
        "information shortcut around destructive or\n",
        "noisy blocks"
      ],
      "metadata": {
        "id": "qiRZI_qwfGEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual block where the number of filters changes\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "# x : (32, 32, 32)\n",
        "residual = x\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "# x : (32, 32, 64)\n",
        "residual = layers.Conv2D(64, 1)(residual)\n",
        "# 차원이 달라 계산할 수 없으므로 1X1 Conv2D layer를 이용한다.\n",
        "x = layers.add([x, residual])"
      ],
      "metadata": {
        "id": "MZRoQ84YtU8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the block includes maxpooling layer\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "# x : (32, 32, 32)\n",
        "residual = x\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
        "# x : (16, 16, 64)\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual)  # apply Conv2D of 1X1 filter.\n",
        "# (16, 16, 64)\n",
        "x = layers.add([x, residual])\n"
      ],
      "metadata": {
        "id": "Dfo0-Qv0iq3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch normalization"
      ],
      "metadata": {
        "id": "Y0SsuDqRkdNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Internal Covariate Shift : distribution change of each layer’s inputs during\n",
        "training as the parameters of the previous layers change.\n",
        "\n",
        "* Inputs to each layer are a ected by the parameters of all preceding layers so that small changes to the network parameters amplify as the network becomes deeper\n",
        "\n",
        "* This requires a lower learning rate and careful parameter initialization, which slows down training and makes it notoriously hard to train models with saturating nonlinearities."
      ],
      "metadata": {
        "id": "ZGjb8MHRkdH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BN transform can be freely added to any subset of activations to be normalized."
      ],
      "metadata": {
        "id": "G5wcG27ul3QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author generally recommend placing the previous layer’s activation after\n",
        "the batch normalization layer (although this is still a subject of debate)"
      ],
      "metadata": {
        "id": "pUWLZkV4mMkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Conv2D(32, 3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x) # activation 을 batch 이후에 두는 것을 추천"
      ],
      "metadata": {
        "id": "vrGlraAfkT66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래와 같이 작성할 수도 있지만 위를 추천\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)"
      ],
      "metadata": {
        "id": "YXDX0PPvmwH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depthwise separable convolutions"
      ],
      "metadata": {
        "id": "8rc3JGsrm98U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depthwise\n",
        "separable convolution ( Depthwise Conv + Pointwise Conv ) is used to\n",
        "build a light weight CNN (fewer parameters and multiply adds) for efficient on device\n",
        "intelligence."
      ],
      "metadata": {
        "id": "-MoVHoV0m93i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A mini Xception like model\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
        "\n",
        "for size in [32, 64, 128, 256, 512]:\n",
        "    residual = x\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "    # batch normalization을 해주는 부분\n",
        "    # 결과는 Conv2D가 조금 더 좋지만 속도는 Separable2D가 빠르다.\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "    residual = layers.Conv2D(\n",
        "    size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "FOEa6ryHnA6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JVmlVQU1IoB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4. Interpreting what convnets learn"
      ],
      "metadata": {
        "id": "V_BPzVTYIsFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing intermediate activations"
      ],
      "metadata": {
        "id": "0iRqLjV-N1qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The representations learned by\n",
        "convnets are highly\n",
        "amenable to visualization\n",
        "\n",
        "* Visualizing\n",
        "intermediate convnet outputs\n",
        "* Visualizing\n",
        "convnets filters\n",
        "* Visualizing\n",
        "heatmaps of class activation in an image"
      ],
      "metadata": {
        "id": "nRXX0YNaJBss"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-VHmvBLrWnE"
      },
      "outputs": [],
      "source": [
        "# You can use this to load the file \"convnet_from_scratch_with_augmentation.keras\"\n",
        "# you obtained in the last chapter.\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "rtdMvwauLjke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGaNgVIcrWnF"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx8qu_7IrWnF"
      },
      "source": [
        "**Preprocessing a single image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZWQ_9mnrWnG"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "img_path = keras.utils.get_file(\n",
        "    fname=\"cat.jpg\",\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
        "\n",
        "# convert image to array\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = keras.utils.load_img(\n",
        "        img_path, target_size=target_size)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "img_tensor = get_img_array(img_path, target_size=(180, 180))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rhIptDFrWnH"
      },
      "source": [
        "**Displaying the test picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOt4mUvirWnH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddgj1FdarWnI"
      },
      "source": [
        "**Instantiating a model that returns layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7koCNHurWnI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "layer_outputs = []\n",
        "layer_names = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
        "        layer_outputs.append(layer.output)\n",
        "        layer_names.append(layer.name)\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers"
      ],
      "metadata": {
        "id": "-fx7AZVANG3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_names"
      ],
      "metadata": {
        "id": "Wr6UtPLpL5vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_OcgIltrWnJ"
      },
      "source": [
        "**Using the model to compute layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcbKlCunrWnJ"
      },
      "outputs": [],
      "source": [
        "# feed images to activation model\n",
        "\n",
        "activations = activation_model.predict(img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(activations)\n",
        "\n",
        "# 9 layer가 있기 때문에 9"
      ],
      "metadata": {
        "id": "YbR9ud13Ousj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKDMWMR4rWnJ"
      },
      "outputs": [],
      "source": [
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fxi2ZkBrWnK"
      },
      "source": [
        "**Visualizing the fifth channel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAI-Jw5YrWnK"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 convnet을 거친 이미지\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# maxpooling을 거친 이미지\n",
        "plt.matshow(activations[1][0, :, :, 5], cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "LTn17y8nQKz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 번째 convnet을 거친 이미지\n",
        "# deeper convnet activations are more abstract\n",
        "plt.matshow(activations[2][0, :, :, 5], cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "CQ3vhTXTQghM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 activation\n",
        "plt.matshow(activations[8][0, :, :, 5], cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "S_xtJhMqRGek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2raE5zxErWnK"
      },
      "source": [
        "**Visualizing every channel in every intermediate activation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eujrDmP4rWnK"
      },
      "outputs": [],
      "source": [
        "# activation output을 visualize해주는 코드(생략)\n",
        "\n",
        "images_per_row = 16\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    n_features = layer_activation.shape[-1]\n",
        "    size = layer_activation.shape[1]\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros(((size + 1) * n_cols - 1,\n",
        "                             images_per_row * (size + 1) - 1))\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_index = col * images_per_row + row\n",
        "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
        "            if channel_image.sum() != 0:\n",
        "                channel_image -= channel_image.mean()\n",
        "                channel_image /= channel_image.std()\n",
        "                channel_image *= 64\n",
        "                channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
        "            display_grid[\n",
        "                col * (size + 1): (col + 1) * size + col,\n",
        "                row * (size + 1) : (row + 1) * size + row] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")\n",
        "\n",
        "    # relu를 거치기 때문에 갈수록 드랍되는 레이어가 많아진다.\n",
        "    # 위의 레이어일수록 고양이의 모습이 많이 남아있다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Things to note:\n",
        "•\n",
        "First layer acts as a collection of various edge detectors,\n",
        "activations retain almost all of the information present in\n",
        "the initial picture\n",
        "\n",
        "•\n",
        "As you go higher, the activations become increasingly\n",
        "abstract and less visually interpretable.\n",
        "\n",
        "•\n",
        "The sparsity of the activations increases with the depth of\n",
        "the layer"
      ],
      "metadata": {
        "id": "aLRmc_OeOgM4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaBHT38mrWnL"
      },
      "source": [
        "### Visualizing convnet filters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the visual pattern that each filter is meant to respond to\n",
        "\n",
        "To maximize the response of a specific filter"
      ],
      "metadata": {
        "id": "XfBJbC9kRmve"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG8Omh-HrWnL"
      },
      "source": [
        "**Instantiating the Xception convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ChG3Yb8rWnL"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmdCyH4ZrWnM"
      },
      "source": [
        "**Printing the names of all convolutional layers in Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZRMKw2OrWnM"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
        "        print(layer.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykr7x01NrWnM"
      },
      "source": [
        "**Creating a feature extractor model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIvJOBR6rWnM"
      },
      "outputs": [],
      "source": [
        "layer_name = \"block3_sepconv1\"\n",
        "layer = model.get_layer(name=layer_name)\n",
        "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaCYxLVsrWnN"
      },
      "source": [
        "**Using the feature extractor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvp1jDRLrWnN"
      },
      "outputs": [],
      "source": [
        "activation = feature_extractor(\n",
        "    keras.applications.xception.preprocess_input(img_tensor)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_tensor.shape"
      ],
      "metadata": {
        "id": "qLOplqQqhNkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation.shape"
      ],
      "metadata": {
        "id": "QZ7Sfz_XhQgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkZ0qfsbrWnN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def compute_loss(image, filter_index):\n",
        "    activation = feature_extractor(image)\n",
        "    filter_activation = activation[:, 2:-2, 2:-2, filter_index] # 테두리 제거\n",
        "    return tf.reduce_mean(filter_activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3KHL-IMrWnN"
      },
      "source": [
        "**Loss maximization via stochastic gradient ascent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_tlYWXDrWnN"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def gradient_ascent_step(image, filter_index, learning_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        loss = compute_loss(image, filter_index)\n",
        "    grads = tape.gradient(loss, image)\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    image += learning_rate * grads\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJUkpHVmrWnO"
      },
      "source": [
        "**Function to generate filter visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPYqZYlqrWnO"
      },
      "outputs": [],
      "source": [
        "img_width = 200\n",
        "img_height = 200\n",
        "\n",
        "def generate_filter_pattern(filter_index):\n",
        "    iterations = 30\n",
        "    learning_rate = 10.\n",
        "    image = tf.random.uniform(\n",
        "        minval=0.4,\n",
        "        maxval=0.6,\n",
        "        shape=(1, img_width, img_height, 3))\n",
        "    for i in range(iterations):\n",
        "        image = gradient_ascent_step(image, filter_index, learning_rate)\n",
        "    return image[0].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiW7Dki_rWnO"
      },
      "source": [
        "**Utility function to convert a tensor into a valid image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl3KJ8JSrWnO"
      },
      "outputs": [],
      "source": [
        "def deprocess_image(image):\n",
        "    image -= image.mean()\n",
        "    image /= image.std()\n",
        "    image *= 64\n",
        "    image += 128\n",
        "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
        "    image = image[25:-25, 25:-25, :]\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI1xGHahrWnO"
      },
      "outputs": [],
      "source": [
        "plt.axis(\"off\")\n",
        "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))\n",
        "\n",
        "# 뒤로 갈수록 복잡해진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LG8-24vrWnP"
      },
      "source": [
        "**Generating a grid of all filter response patterns in a layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1xOmey2rWnP"
      },
      "outputs": [],
      "source": [
        "all_images = []\n",
        "for filter_index in range(64):\n",
        "    print(f\"Processing filter {filter_index}\")\n",
        "    image = deprocess_image(\n",
        "        generate_filter_pattern(filter_index)\n",
        "    )\n",
        "    all_images.append(image)\n",
        "\n",
        "margin = 5\n",
        "n = 8\n",
        "cropped_width = img_width - 25 * 2\n",
        "cropped_height = img_height - 25 * 2\n",
        "width = n * cropped_width + (n - 1) * margin\n",
        "height = n * cropped_height + (n - 1) * margin\n",
        "stitched_filters = np.zeros((width, height, 3))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        image = all_images[i * n + j]\n",
        "        stitched_filters[\n",
        "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
        "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
        "            + cropped_height,\n",
        "            :,\n",
        "        ] = image\n",
        "\n",
        "keras.utils.save_img(\n",
        "    f\"filters_for_layer_{layer_name}.png\", stitched_filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nevojy8ZrWnP"
      },
      "source": [
        "### Visualizing heatmaps of class activation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which parts of a given image led a convnet to its final\n",
        "classification decision\n",
        "\n",
        "producing heatmaps of class activation over input images"
      ],
      "metadata": {
        "id": "hmDpqYXJoXti"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKorRbiQrWnP"
      },
      "source": [
        "**Loading the Xception network with pretrained weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMmsy33ZrWnP"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception(weights=\"imagenet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alzy3YrQrWnQ"
      },
      "source": [
        "**Preprocessing an input image for Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqwA14FQrWnQ"
      },
      "outputs": [],
      "source": [
        "# download the elephant images\n",
        "img_path = keras.utils.get_file(\n",
        "    fname=\"elephant.jpg\",\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n",
        "\n",
        "# convert image to array\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    array = keras.applications.xception.preprocess_input(array)\n",
        "    return array\n",
        "\n",
        "img_array = get_img_array(img_path, target_size=(299, 299))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " img_array.shape"
      ],
      "metadata": {
        "id": "oSB31cO9wFxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(keras.utils.load_img(img_path, target_size=(299, 299)))"
      ],
      "metadata": {
        "id": "R7vGgRs9wS3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge_lhwmOrWnQ"
      },
      "outputs": [],
      "source": [
        "# prediction\n",
        "\n",
        "preds = model.predict(img_array)\n",
        "print(keras.applications.xception.decode_predictions(preds, top=3)[0])\n",
        "\n",
        "# afican elephant일 가능성이 87%로 가장 높다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCxVSqUprWnQ"
      },
      "outputs": [],
      "source": [
        "np.argmax(preds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize which parts of the image are the most African elephant like, let’s set up the Grad CAM process"
      ],
      "metadata": {
        "id": "FEIwyT9Gpt4I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APkHbBd1rWnQ"
      },
      "source": [
        "**Setting up a model that returns the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6QQIn33rWnQ"
      },
      "outputs": [],
      "source": [
        "# Create a model that maps the input image to the activations of the last convolutional layer.\n",
        "\n",
        "last_conv_layer_name = \"block14_sepconv2_act\"\n",
        "classifier_layer_names = [\n",
        "    \"avg_pool\",\n",
        "    \"predictions\",\n",
        "]\n",
        "last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUr5PPIZrWnR"
      },
      "source": [
        "**Reapplying the classifier on top of the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_kZtWD-rWnR"
      },
      "outputs": [],
      "source": [
        "# Create a model that maps the activations of the last convolutional layer to the final class predictions.\n",
        "\n",
        "classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "x = classifier_input\n",
        "for layer_name in classifier_layer_names:\n",
        "    x = model.get_layer(layer_name)(x)\n",
        "classifier_model = keras.Model(classifier_input, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y0uP7UFrWnR"
      },
      "source": [
        "**Retrieving the gradients of the top predicted class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWskKNBPrWnR"
      },
      "outputs": [],
      "source": [
        "# Compute the gradient of the top predicted class for our input image with respect to the activations of the last convolution layer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "    tape.watch(last_conv_layer_output)\n",
        "    preds = classifier_model(last_conv_layer_output)\n",
        "    top_pred_index = tf.argmax(preds[0])\n",
        "    top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Sx5yPsrWnR"
      },
      "source": [
        "**Gradient pooling and channel-importance weighting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF2NjEJ3rWnR"
      },
      "outputs": [],
      "source": [
        "# Apply pooling and importance weighting to the gradient tensor to obtain our heatmap of class activation\n",
        "\n",
        "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()\n",
        "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "for i in range(pooled_grads.shape[-1]):\n",
        "    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "heatmap = np.mean(last_conv_layer_output, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjHkUK2IrWnS"
      },
      "source": [
        "**Heatmap post-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aBSIFHUrWnS"
      },
      "outputs": [],
      "source": [
        "# For visualization purposes, we’ll also normalize the heatmap between 0 and 1.\n",
        "\n",
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sFV9x_SrWnS"
      },
      "source": [
        "**Superimposing the heatmap on the original picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44qU7yn1rWnS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "img = keras.utils.load_img(img_path)\n",
        "img = keras.utils.img_to_array(img)\n",
        "\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "jet = cm.get_cmap(\"jet\")\n",
        "jet_colors = jet(np.arange(256))[:, :3]\n",
        "jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "superimposed_img = jet_heatmap * 0.4 + img\n",
        "superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "save_path = \"elephant_cam.jpg\"\n",
        "superimposed_img.save(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(superimposed_img)"
      ],
      "metadata": {
        "id": "pkNji_MY0jrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OptRpEXA0nIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V6TKx1XOJk_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lJVRsl8Jcak"
      },
      "source": [
        "# Ch10. Deep learning for timeseries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Different kinds of\n",
        "timeseries tasks\n",
        "\n",
        "* A temperature\n",
        "forecasting example\n",
        "\n",
        "* Understanding recurrent neural networks\n",
        "\n",
        "* Advanced use of recurrent neural networks"
      ],
      "metadata": {
        "id": "RhoMm8hZJ4Wo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjRNHLv8Jcal"
      },
      "source": [
        "## 10.1. Different kinds of timeseries tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Forecasting : predicting what will happen next in a series\n",
        "* Classification\n",
        ": Assign one or more categorical labels to a\n",
        "timeseries .\n",
        "* Event detection\n",
        ": Identify the occurrence of a specific\n",
        "expected event within a continuous data stream\n",
        "* Anomaly detection\n",
        ": Detect anything unusual happening\n",
        "within a continuous datastream"
      ],
      "metadata": {
        "id": "L5PzK2PCJ_Yz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr16DEAbJcal"
      },
      "source": [
        "## 10.2. A temperature-forecasting example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weather timeseries dataset recorded at the weather station\n",
        "at the Max Planck Institute for Biogeochemistry in Jena,\n",
        "Germany"
      ],
      "metadata": {
        "id": "2XQ8a_H8KVX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features:\n",
        "14 different quantities (such as temperature,\n",
        "pressure, humidity, wind direction, and so on) were recorded\n",
        "every 10 minutes over 2009~2016"
      ],
      "metadata": {
        "id": "WbQ-AtzoKTgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eZ0II2pJcam"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "8ey30QZ_KKgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head jena_climate_2009_2016.csv"
      ],
      "metadata": {
        "id": "g7PZD4fQKWcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s53akp7tJcan"
      },
      "source": [
        "**Inspecting the data of the Jena weather dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjOsGS6hJcan"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "fname = os.path.join(\"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUk4KtyKJcao"
      },
      "source": [
        "**Parsing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7jZQLCXJcao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1-tWGJGJcap"
      },
      "source": [
        "**Plotting the temperature timeseries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-pqDxlIJcaq"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(range(len(temperature)), temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqJEmwP5Jcaq"
      },
      "source": [
        "**Plotting the first 10 days of the temperature timeseries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdb2Yls_Jcaq"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1440), temperature[:1440])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hhosjSlJcar"
      },
      "source": [
        "**Computing the number of samples we'll use for each data split**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use\n",
        "the first 50% of the data for training, the following 25 %\n",
        "for validation, and the last 25% for testing"
      ],
      "metadata": {
        "id": "mbZj2wyg7vGA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HGYzoddJcar"
      },
      "outputs": [],
      "source": [
        "num_train_samples = int(0.5 * len(raw_data))\n",
        "num_val_samples = int(0.25 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "print(\"num_train_samples:\", num_train_samples)\n",
        "print(\"num_val_samples:\", num_val_samples)\n",
        "print(\"num_test_samples:\", num_test_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlMslT-EJcar"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN6IvieaJcar"
      },
      "source": [
        "**Normalizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDbNjlGSJcas"
      },
      "outputs": [],
      "source": [
        "# 타임시리즈 데이터에서는 mean과 SD를 통해 normalization을 하는 경우가 많다.\n",
        "# normalize하는 데에는 train dataset만 사용\n",
        "\n",
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "timeseries_dataset_from_array\n",
        "()의 사용방법\n",
        "\n",
        "timeseries_dataset_from_array\n",
        "() gives you windows extracted from the original\n",
        "timeseries (we’ll call them “sequences)"
      ],
      "metadata": {
        "id": "Y8pFUtKZ8fS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16JSmEWKJcas"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "int_sequence = np.arange(10)\n",
        "dummy_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    data=int_sequence[:-3], # 마지막 3개는 target info가 없어서 사용하지 않음.\n",
        "    targets=int_sequence[3:],\n",
        "    sequence_length=3,\n",
        "    batch_size=2,\n",
        ")\n",
        "\n",
        "for inputs, targets in dummy_dataset:\n",
        "    for i in range(inputs.shape[0]):\n",
        "        print([int(x) for x in inputs[i]], int(targets[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqMuzZvqJcas"
      },
      "source": [
        "**Instantiating datasets for training, validation, and testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리 데이터에 timeseries_dataset_from_array 적용"
      ],
      "metadata": {
        "id": "hQ30OGU8MEE4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5lojHIyJcat"
      },
      "outputs": [],
      "source": [
        "sampling_rate = 6 # one point per hour\n",
        "sequence_length = 120 # observation will go back 5 days(120hour)(weekly pattern과 관계있다면 7일을 기준으로 쓰는 것이 좋다.)\n",
        "delay = sampling_rate * (sequence_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples)\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples)\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut0Q4Ba7Jcat"
      },
      "source": [
        "**Inspecting the output of one of our datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CpbAhTJcat"
      },
      "outputs": [],
      "source": [
        "for samples, targets in train_dataset:\n",
        "    print(\"samples shape:\", samples.shape)\n",
        "    print(\"targets shape:\", targets.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXxbCtaKJcau"
      },
      "source": [
        "### A common-sense, non-machine-learning baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEPjRu4GJcau"
      },
      "source": [
        "**Computing the common-sense baseline MAE**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a common sense approach is\n",
        "to always predict that the temperature 24 hours from now will be equal to the\n",
        "temperature right now."
      ],
      "metadata": {
        "id": "ajS8v2enIloJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcuSPgmmJcau"
      },
      "outputs": [],
      "source": [
        "def evaluate_naive_method(dataset):\n",
        "    total_abs_err = 0.\n",
        "    samples_seen = 0\n",
        "    for samples, targets in dataset:\n",
        "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
        "        total_abs_err += np.sum(np.abs(preds - targets))\n",
        "        samples_seen += samples.shape[0]\n",
        "    return total_abs_err / samples_seen\n",
        "\n",
        "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
        "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")\n",
        "\n",
        "# common sense의 결과는 2.62"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOI2Z02bJcau"
      },
      "source": [
        "### Let's try a basic machine-learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTE14ueJcau"
      },
      "source": [
        "**Training and evaluating a densely connected model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDuZx2-ZJcav"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "# regression problem이기 때문에 activation function은 존재하지 않는다.\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# save the best performing model\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_dense.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# reload the best model and evaluate it on the test data\n",
        "model = keras.models.load_model(\"jena_dense.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")\n",
        "\n",
        "# dense model의 결과는 2.70. 위와 크게 다르지 않다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcNyCDpAJcav"
      },
      "source": [
        "**Plotting results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GINhXO67Jcav"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss = history.history[\"mae\"]\n",
        "val_loss = history.history[\"val_mae\"]\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
        "plt.title(\"Training and validation MAE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jcZTrD-Jcav"
      },
      "source": [
        "### Let's try a 1D convolutional model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv1D layer relies on 1D windows that slide across input sequences. They’re a\n",
        "great fit for any sequence data that follows the translation invariance assumption"
      ],
      "metadata": {
        "id": "KjnyTVf-NX8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7W-fxPiJcav"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Conv1D(8, 24, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 12, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 6, activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_conv.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_conv.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")\n",
        "\n",
        "# 1D convolution의 결과는 3.19. 매우 안좋다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D convolutional\n",
        "model performs even worse than the densely connected model\n",
        "\n",
        "Weather data doesn’t quite respect the translation invariance assumption: data\n",
        "from a morning follows different properties than data from an evening\n",
        "\n",
        "Order in our data matters a lot: The recent past is far more informative for\n",
        "predicting the next day’s temperature than data from five days ago\n",
        "\n",
        "Common\n",
        "sense baseline: Validation MAE (2.44), Test MAE (2.62)"
      ],
      "metadata": {
        "id": "YLa0Z2nsODjX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLW22r0QJcaw"
      },
      "source": [
        "### A first recurrent baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Densely connected approach removed the notion of time from the input data.\n",
        "\n",
        "The\n",
        "convolutional approach treated every segment of the data in the same way\n",
        "destroyed order information"
      ],
      "metadata": {
        "id": "6NZttqdjOvpX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KxoSNfPJcaw"
      },
      "source": [
        "**A simple LSTM-based model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqZ4o-v3Jcaw"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(16)(inputs) # LSTM layer\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_lstm.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")\n",
        "\n",
        "# LSTM의 결과는 2.58. 아주 조금 좋아졌다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jc2W-whJcaw"
      },
      "source": [
        "## 10.3. Understanding recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As\n",
        "you’re reading the present sentence, you’re processing it word by word. A\n",
        "recurrent neural network (RNN) adopts the same principle."
      ],
      "metadata": {
        "id": "idyoe3UWPHFf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwbSqANJJcaw"
      },
      "source": [
        "**NumPy implementation of a simple RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7pAIWnGJcaw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 100\n",
        "input_features = 32\n",
        "output_features = 64\n",
        "inputs = np.random.random((timesteps, input_features))\n",
        "state_t = np.zeros((output_features,))\n",
        "W = np.random.random((output_features, input_features))\n",
        "U = np.random.random((output_features, output_features))\n",
        "b = np.random.random((output_features,))\n",
        "successive_outputs = []\n",
        "for input_t in inputs:\n",
        "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
        "    successive_outputs.append(output_t)\n",
        "    state_t = output_t\n",
        "final_output_sequence = np.stack(successive_outputs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wIXDmrBJcax"
      },
      "source": [
        "### A recurrent layer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8td5EHWjJcax"
      },
      "source": [
        "**An RNN layer that can process sequences of any length**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs of shape (\n",
        "batch_size , timesteps , input_features ), you can set the timesteps\n",
        "entry to None, which enables your network to process sequences of arbitrary length."
      ],
      "metadata": {
        "id": "fybU-CiEIWrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfSjubK9Jcax"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "num_features = 14\n",
        "inputs = keras.Input(shape=(None, num_features))\n",
        "outputs = layers.SimpleRNN(16)(inputs) # simpleRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XcVfX8rJcax"
      },
      "source": [
        "**An RNN layer that returns only its last output step**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All recurrent layers in\n",
        "Keras (SimpleRNN , LSTM, and GRU) can be run in two different\n",
        "modes:\n",
        "\n",
        "return full sequences of successive outputs for each timestep (a rank 3 tensor of shape\n",
        "batch_size , timesteps , output_features ))\n",
        "\n",
        "or return only the last output for each input sequence\n",
        "(a rank 2 tensor of shape ( batch_size , output_features))"
      ],
      "metadata": {
        "id": "7xtBiGztJo7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpVC2hG9Jcax"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences=False)(inputs) # return_sequence=False가 디폴트값. False면 마지막 정보만 돌려준다.\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7GOhrZfJcax"
      },
      "source": [
        "**An RNN layer that returns its full output sequence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66U6SB4XJcay"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences=True)(inputs) # return_sequences만 True로 바꿔줌.\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmLBkpUCJcay"
      },
      "source": [
        "**Stacking RNN layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCA5Fm23Jcay"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "x = layers.SimpleRNN(16, return_sequences=True)(inputs)\n",
        "x = layers.SimpleRNN(16, return_sequences=True)(x)\n",
        "outputs = layers.SimpleRNN(16)(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90OdDs-8Jcay"
      },
      "source": [
        "## 10.4. Advanced use of recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66R30_IGJcay"
      },
      "source": [
        "### Using recurrent dropout to fight overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZujDoInJca0"
      },
      "source": [
        "**Training and evaluating a dropout-regularized LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjAGW8bhJca1"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.25)(inputs) # recurrent_dropout=0.25 만 적어주면 된다.\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# To refularize the Dense layer, we also add a Dropout layer after LSTM\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# 시간 많이 걸림."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt5ux59_Jca1"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, num_features))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.2, unroll=True)(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w37Zm5AJca1"
      },
      "source": [
        "### Stacking recurrent layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DziIAKQEJca1"
      },
      "source": [
        "**Training and evaluating a dropout-regularized, stacked GRU model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZhOOnjYJca1"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "# recurrent layer를 쌓기 위해 return_sequences = True로 함.\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "# GRU 사용. LSTM의 간단한 버전.\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")\n",
        "\n",
        "# 시간 많이 걸림.\n",
        "# 결과가 제일 좋다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P4hResPJca1"
      },
      "source": [
        "### Using bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWSXI21SJca2"
      },
      "source": [
        "**Training and evaluating a bidirectional LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PKNyfSOJca2"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Bidirectional(layers.LSTM(16))(inputs) # bidirectional LSTM model\n",
        "outputs = layers.Dense(1)(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset)\n",
        "\n",
        "# 이 문제에는 아니지만 bidirectional이 best model이 되는 경우도 많다.\n",
        "# 여러 모델을 시도해보고 가장 결과가 좋은 것을 채택할 것."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXmgT19i1iSW"
      },
      "source": [
        "# Ch11. Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn6B9uHq1iSX"
      },
      "source": [
        "## 11.1. Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Language is how we store almost all of our knowledge.\n",
        "\n",
        "* However, the ability to understand natural language has long eluded machines\n",
        "\n",
        "* 1960s: the famous ELIZA program used pattern matching to sustain very\n",
        "basic conversation\n",
        "\n",
        "* 1960s-1990s: Handcrafted rules held out as the dominant approach\n",
        "\n",
        "\n",
        "* Late 1980s: started seeing machine learning approaches to NLP\n",
        "\n",
        "* modern NLP is about using machine learning and large datasets to give\n",
        "computers the ability to understand language\n",
        "* 1990s to the early 2010s:\n",
        "The toolset of NLP decision trees, logistic\n",
        "regression only saw slow evolution.\n",
        "* Most of the research focus was on feature engineering\n",
        "* 2014-2015: Multiple researchers began to investigate the language\n",
        "understanding capabilities of recurrent neural networks , LSTM\n",
        "* 2015-2017: recurrent neural networks dominated the booming NLP scene: BiLSTM\n",
        "* 2017-2018: new architecture rose to replace RNNs: the **Transformer** , these days\n",
        "most NLP systems are based on them"
      ],
      "metadata": {
        "id": "wFVKvU206tx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP topics\n",
        "\n",
        "\n",
        "* Text classification\n",
        ": “What’s the topic of this text?”\n",
        "* Content filtering\n",
        ": “Does this text contain abuse?”\n",
        "* Sentiment analysis\n",
        ": “Does this text sound positive or negative?”\n",
        "* Language modeling\n",
        ": “What should be the next word in this incomplete\n",
        "sentence?”\n",
        "\n",
        "* Translation\n",
        ": “How would you say this in German?”\n",
        "* Summarization\n",
        ": “How would you summarize this article in one"
      ],
      "metadata": {
        "id": "zLavtGuX72Ew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxV4c-FV1iSX"
      },
      "source": [
        "## 11.2. Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like all other neural networks, deep\n",
        "learning models don’t\n",
        "take as input raw text: they only work with numeric tensors\n",
        "\n",
        "Vectorizing\n",
        "text is the process of transforming text into\n",
        "numeric tensors\n",
        "\n",
        "* **Standardize** the text to make it easier to process, such as by converting it\n",
        "to lowercase or removing punctuation.\n",
        "\n",
        "* Split the text into units (called tokens ), such as characters, words, or\n",
        "groups of words. This is called **tokenization**\n",
        "\n",
        "* Convert each such token into a numerical vector. This will usually involve\n",
        "first **indexing** all tokens present in the data."
      ],
      "metadata": {
        "id": "JVh1AHt68Kf_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKLAuZ3I1iSX"
      },
      "source": [
        "### 11.2.1. Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider these two sentences:\n",
        "\n",
        "    “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”\n",
        "\n",
        "    “Sunset came; I stared at the México sky. Isn’t nature splendid?”\n",
        "\n",
        "Text standardization is a basic form of feature engineering\n",
        "that aims to erase encoding differences\n",
        "\n",
        "Simplest and most widespread standardization schemes is\n",
        "\n",
        "\n",
        "\"convert to lowercase and remove punctuation\"\n",
        "\n",
        "    “sunset came i was staring at the mexico sky isnt nature splendid”\n",
        "    “sunset came i stared at the méxico sky isnt nature splendid”\n",
        "\n",
        "Another common transformation is to convert special characters\n",
        "to a standard form, such as replacing “é” with “e,” “æ” with “ae,”\n",
        "and so on.\n",
        "\n",
        "\n",
        "**stemming**\n",
        ": converting variations of a term (such as different\n",
        "conjugated forms of a verb) into a single shared representation,\n",
        "like turning “caught” and “been catching” into “[catch]” or “cats”\n",
        "into “[cat]”.\n",
        "\n",
        "    “sunset came i [stare] at the mexico sky isnt nature splendid”\n",
        "\n",
        "\n",
        "With these standardization techniques, your model will require\n",
        "less training data and will generalize better"
      ],
      "metadata": {
        "id": "moM4sgmaJNhc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_NZ2kya1iSY"
      },
      "source": [
        "### 11.2.2. Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the text into units (called\n",
        "tokens ), such as characters, words,\n",
        "or groups of words. This is called tokenization\n",
        "\n",
        "* Word level tokenization :  Where tokens are space separated substrings. A\n",
        "variant of this is to further split words into subwords for instance,\n",
        "treating “staring” as “ star+ing ” or “called” as call+ed\n",
        "\n",
        "* N gram tokenization : Where tokens are groups of N consecutive words.\n",
        "For instance, “the cat” or “he was” would be 2 gram tokens\n",
        "\n",
        "* Character level tokenization :  Where each character is its own token. In\n",
        "practice, this scheme is rarely used, and you only really see it in\n",
        "specialized contexts, like text generation or speech recognition. 거의 사용되지 않음.\n",
        "\n",
        "There are two kinds of text\n",
        "processing models: those that care about\n",
        "word order, called **sequence models** , and those that treat input words\n",
        "as a set, discarding their original order, called **bag of words models**\n",
        "\n"
      ],
      "metadata": {
        "id": "52uDHVNiKP4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word\n",
        "n grams are groups of N (or fewer) consecutive words\n",
        "that you can extract from a sentence. 순서가 중요함. n의 크기에 따라 얼마나 순서가 유지될지 결정됨\n",
        "\n",
        "bag-of-words : 순서가 사라짐"
      ],
      "metadata": {
        "id": "Fsp2wTB4KyuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag\n",
        "of words isn’t an order preserving tokenization method\n",
        "\n",
        "General structure of the sentences is lost\n",
        "\n",
        "Tends to be used in shallow language\n",
        "processing models\n",
        "\n",
        "Extracting n\n",
        "grams is a form of feature engineering, and deep\n",
        "learning models (RNN, 1D CNN, and Transformer) do away\n",
        "with this manual approach."
      ],
      "metadata": {
        "id": "AK72yPAlLL1G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV2-GFAr1iSZ"
      },
      "source": [
        "### 11.2.3. Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to encode each token into a numerical representation\n",
        "\n",
        "Build an index of all terms found in the training data\n",
        "\n",
        "convert that integer into a vector encoding. Ex) One\n",
        "hot"
      ],
      "metadata": {
        "id": "eROSdSGpLUKW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUOMTNDb1iSa"
      },
      "source": [
        "### Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3KpiVjA1iSa"
      },
      "outputs": [],
      "source": [
        "# simple implement using pure Python\n",
        "\n",
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.vocabulary"
      ],
      "metadata": {
        "id": "fNrypBg_CX9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhUAPdi51iSc"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence) # test_sentence를 우리가 만든 vectorizer로 encoding.\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTo7jmRn1iSd"
      },
      "outputs": [],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence) # encoded_sentence를 다시 decoding.\n",
        "print(decoded_sentence)\n",
        "\n",
        "# still은 데이터에 없었기 때문에 unkown: UNK로 표시됨"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, you’ll work with the\n",
        "Keras TextVectorization layer\n",
        "\n",
        "you can provide custom functions for standardization and tokenization, which\n",
        "means the layer is flexible enough to handle any use case. Note that such custom\n",
        "functions should operate on tf.string tensors"
      ],
      "metadata": {
        "id": "Dozc64C9PW3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvaL89Yg1iSd"
      },
      "outputs": [],
      "source": [
        "# keras에 textvectorizor layer가 존재. 알아서 바꿔줌. 위에처럼 직접 구현할 필요가 없음.\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9qepTDv1iSd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor) # tf.string임.\n",
        "    # convert strings to lowercase\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "    # replace punctuation characters with the empty string\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "    # split strings on whitespace.\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZvje5941iSe"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)\n",
        "# To index the vocabulary of a text corpus, just call the adapt() method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcE4vjx21iSe"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KJwSiNP1iSe"
      },
      "outputs": [],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nget6T051iSf"
      },
      "outputs": [],
      "source": [
        "# encoding\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka-IBX3v1iSf"
      },
      "outputs": [],
      "source": [
        "# decoding\n",
        "\n",
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab"
      ],
      "metadata": {
        "id": "_qS_1jtlKXkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to put in the tf.data pipeline, like this:\n",
        "\n",
        "int_sequence_dataset = string_dataset.map(\n",
        "    text_vectorization,\n",
        "    num_parallel_calls=4\n",
        ")"
      ],
      "metadata": {
        "id": "VbHIlAeWe8-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bEgt3cV1iSf"
      },
      "source": [
        "## 11.3. Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bag of words models : simplest thing you could do is just discard order and\n",
        "treat text as an unordered set of words(순서 고려하지 않음)\n",
        "\n",
        "Sequence models\n",
        ": take into account word order(순서 고려)\n",
        "* RNNs: words should be processed strictly in the order in which they\n",
        "appear, one at a time\n",
        "* Transformer\n",
        ": technically order agnostic, yet it injects word position\n",
        "information into the representations it processes"
      ],
      "metadata": {
        "id": "KUBqyX9tfejj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d6P0cOX1iSf"
      },
      "source": [
        "### 11.3.1. Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrL-bei31iSg"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "“train/\n",
        "pos /” directory contains a set of 12,500 text files with positive\n",
        "reviews, negative sentiment reviews live in the “train/ neg /” directory. In\n",
        "total, there are 25,000 text files"
      ],
      "metadata": {
        "id": "VhpV3NHogHvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "iIszvnnhLDjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"aclImdb\")"
      ],
      "metadata": {
        "id": "2dgQxLulLHxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs84VbSv1iSg"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLmeH8EP1iSg"
      },
      "outputs": [],
      "source": [
        "# example of dataset\n",
        "\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZzyMOPe1iSg"
      },
      "outputs": [],
      "source": [
        "# Prepare a validation set by setting apart 20% of the training text files in a new directory, aclImdb/val\n",
        "\n",
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQxIttXW1iSh"
      },
      "outputs": [],
      "source": [
        "# text_dataset_from_directory\n",
        "\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HM-2Zzp1iSh"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdsbrkAZ1iSh"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCMtCSZG1iSh"
      },
      "source": [
        "### 11.3.2. Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AfJtFnf1iSh"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJDFdz5l1iSh"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t85Pj6bf1iSi"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000, # 가장 자주 등장하는 단어 20000개만.\n",
        "    output_mode=\"multi_hot\", # output을 multi-hot binary vector로 바꿈.\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "# prepare a dataset that only yields raw text inputs(no labels)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "# Use that dataset to index the dataset vocabulary via the adapt() method.\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()\n",
        "# 20000개"
      ],
      "metadata": {
        "id": "iizXX762qYwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_only_train_ds.take(1):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "yfaGsC4ypSpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, y in train_ds.take(1):\n",
        "    print(i)\n",
        "    print(y)"
      ],
      "metadata": {
        "id": "ZtMQbPiMqHRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyc7ozE21iSi"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9OZdkhU1iSi"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDBvZJ71iSi"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sgALThb1iSi"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x) # activation : sigmoid\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvmLGtuS1iSj"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oERB9xgU1iSj"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axmk_nLi1iSj"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkdU3AXN1iSj"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a2VOYrL1iSk"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2, # binary encoding이기 때문에 ngrams = 2\n",
        "    max_tokens=20000, # 가장 자주 사용되는 20000개\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4alfHcJ1iSk"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91ZhRU_f1iSk"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")\n",
        "\n",
        "# unigram을 사용한 것보다 결과가 약간 더 좋다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 자주 사용되는 20000개 단어 뽑기\n",
        "text_vectorization.get_vocabulary"
      ],
      "metadata": {
        "id": "FsA-DZWeURj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koHkBa2H1iSk"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArIK0FYJ1iSl"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram with count encoding\n",
        "\n",
        "\n",
        "Add a bit more information to this representation by counting how many\n",
        "times each word or N\n",
        "gram occurs"
      ],
      "metadata": {
        "id": "xRMgJ1yEVTjT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNGD-gld1iSl"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\" # output mode를 multi hot이 아니라 count로 두면 등장한 횟수만큼 수를 부여한다.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMjyj52X1iSl"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Terms that appear in almost every document (like “the” or “a”) aren’t\n",
        "particularly informative\n",
        "\n",
        "* Terms that appear only in a small subset of all texts (like “Herzog”) are\n",
        "very distinctive, and thus important\n",
        "\n",
        "* TF-IDF is a metric that fuses these two ideas. It weights a given term by\n",
        "taking “term frequency,” how many times the term appears in the current\n",
        "document, and dividing it by a measure of “document frequency,” which\n",
        "estimates how often the term comes up across the dataset."
      ],
      "metadata": {
        "id": "M-Des82pVyZT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFU_TZpr1iSl"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\", # output mode를 tf_idf로 두면 된다.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC_TSUBe1iSl"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf_idf는 cpu에서밖에 안돌아가서 코드가 한 줄 더 필요함\n",
        "\n",
        "with tf.device(\"CPU\"):\n",
        "    text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "0nUAP430XneE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V--jOGMc1iSl"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n",
        "\n",
        "# 결과가 직전 것보다는 약간 낮음."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting a model that processes raw strings\n",
        "\n",
        "* If we want to export a standalone model independent of this pipeline, we\n",
        "should make sure that it incorporates its own text preprocessing\n",
        "\n",
        "* Create a new model that reuses your\n",
        "TextVectorization layer and adds to it\n",
        "the model you just trained:"
      ],
      "metadata": {
        "id": "lhn5syz6We2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80gfyP1C1iSm"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "# apply test preprocessing. 같은 text_vectorization을 사용해야됨.\n",
        "outputs = model(processed_inputs)\n",
        "# apply the previously trained model\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJLe0fVB1iSm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKNc3vYa1it1"
      },
      "source": [
        "### 11.3.3. Processing words as a sequence: The sequence model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN9qOe4U1it2"
      },
      "source": [
        "#### A first practical example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhKLdWHt1it2"
      },
      "source": [
        "**Downloading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASFuxI8B1it2"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkNi53Ik1it4"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWMX7rG01it4"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY1tt5dM1it5"
      },
      "source": [
        "**Preparing integer sequence datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPqZGJcp1it5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization( # TextVectorization layer를 사용\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    # output mode는 int. int로 두면 order information이 사라지지 않는다.\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U8azVMd1it6"
      },
      "source": [
        "**A sequence model built on one-hot encoded vector sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to convert our integer sequences to vector sequences is\n",
        "to one hot encode the. On top of these one hot vectors, we’ll add a simple\n",
        "bidirectional LSTM."
      ],
      "metadata": {
        "id": "SbncIfxHZSWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5W92ZaT1it7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens) # one_hot encoding\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) # bidirectional LSTM\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zhNNvU1it7"
      },
      "source": [
        "**Training a first basic sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBFUct641it7"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "\n",
        "# 결과가 좋지 않다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6JAtmky1it8"
      },
      "source": [
        "#### Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another popular and powerful way to associate a vector with a word\n",
        "is the use of dense word vectors, also called word embeddings\n",
        "* Dense\n",
        "* Lower dimensional\n",
        "* Leamed from data\n",
        "* the geometric relationship between two word\n",
        "vectors should reflect the semantic relationship\n",
        "between these words"
      ],
      "metadata": {
        "id": "H6L9As9SZq6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two ways to obtain word\n",
        "embeddings\n",
        "* Learn word\n",
        "embeddings jointly with the main task you care about\n",
        "(such as document classification or sentiment prediction).\n",
        "* Load into your model word\n",
        "embeddings that were precomputed\n",
        "using a different machine learning task than the one you’re trying to\n",
        "solve. These are called pretrained word embeddings"
      ],
      "metadata": {
        "id": "lTfuzt2sagnI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFDc8M1x1it8"
      },
      "source": [
        "#### Learning word embeddings with the Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1kaQqqh1it8"
      },
      "source": [
        "**Instantiating an `Embedding` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuNd9Swl1it8"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnX0Ge851it9"
      },
      "source": [
        "**Model that uses an `Embedding` layer trained from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDRGIrYT1it9"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs) # embeding layer\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "\n",
        "# 결과가 좋지 않다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FmjTWjH1it9"
      },
      "source": [
        "#### Understanding padding and masking"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Padding\n",
        ": Sentences longer than K tokens are truncated to a length of K\n",
        "tokens, and sentences shorter than K tokens are padded with zeros at the end\n",
        "so that they can be concatenated together with other sequences.\n",
        "We may have too many zeros for shorter sequences. The information stored\n",
        "in the internal state of the RNN will gradually fade out as it gets exposed to\n",
        "these meaningless inputs.\n",
        "* Masking\n",
        ": Tell the RNN that it should skip these iterations of"
      ],
      "metadata": {
        "id": "N_-aKfs7e8o6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rNtttDn1it9"
      },
      "source": [
        "**Using an `Embedding` layer with masking enabled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlrwZM5L1it-"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "    # embeding layer에서 mask_zero=True로 두면 된다.\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "# 결과가 아주 조금 좋아졌다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyuTr6X61it-"
      },
      "source": [
        "#### Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes you have so little training data available that you can’t use your data\n",
        "alone to learn an appropriate task specific embedding of your vocabulary\n",
        "\n",
        "In such cases, instead of learning word\n",
        "embeddings jointly with the problem you\n",
        "want to solve, you can load embedding vectors from a precomputed embedding\n",
        "space that you know is highly structured and exhibits useful properties:\n",
        "Word2Vec, GloVe , etc\n",
        "\n",
        "Word2Vec: https://code.google.com/archive/p/word2vec\n",
        "\n",
        "GloVe\n",
        ": https://nlp.stanford.edu/projects/glove\n",
        "\n",
        "They are precomputed\n",
        "embeddings for millions of English tokens, obtained from\n",
        "Wikipedia data and Common Crawl data."
      ],
      "metadata": {
        "id": "uy7Xdbz8ft-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoKwKq3X1it-"
      },
      "outputs": [],
      "source": [
        "# Download GloVe word embeddings precomputed on the 2014 English Wikipedia dataset:\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "O-nHBC8ToKVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head glove.6B.100d.txt"
      ],
      "metadata": {
        "id": "YC31z4X0oQiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lREKU3l1it-"
      },
      "source": [
        "**Parsing the GloVe word-embeddings file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI5IGBQp1it_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index[\"the\"]"
      ],
      "metadata": {
        "id": "axWvFV8doe3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqGu6uz_1it_"
      },
      "source": [
        "**Preparing the GloVe word-embeddings matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLFDF_zp1it_"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_index)"
      ],
      "metadata": {
        "id": "Jd50rpuFo01B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "id": "JISOw9QNo9W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8PR1l751it_"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained embeddings in an Embedding layer:\n",
        "\n",
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False, # freeze\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTa9KzZ91it_"
      },
      "source": [
        "**Model that uses a pretrained Embedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBqm6j0g1it_"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "# IMDB dataset은 데이터가 충분히 많아서 이 방법이 효과적이지 못함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBeEF1Mw1i6K"
      },
      "source": [
        "## 11.4. The Transformer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPGKyhl1i6K"
      },
      "source": [
        "### Understanding self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfX-hiDv1i6L"
      },
      "source": [
        "#### Generalized self-attention: the query-key-value model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3TmkSYX1i6L"
      },
      "source": [
        "### Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siyq_n8r1i6L"
      },
      "source": [
        "### The Transformer encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1f2B1WN1i6M"
      },
      "source": [
        "**Getting the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSDU9xBB1i6M"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ2XODr-1i6O"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMpVeQ2y1i6O"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvwOtorV1i6P"
      },
      "source": [
        "**Vectorizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3NPVsHp1i6P"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdgGEdDT1i6Q"
      },
      "source": [
        "**Transformer encoder implemented as a subclassed `Layer`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG_WtK-p1i6Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d-WceF-1i6R"
      },
      "source": [
        "**Using the Transformer encoder for text classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhWMM-cB1i6R"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLthdW1l1i6S"
      },
      "source": [
        "**Training and evaluating the Transformer encoder based model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz5GlEx01i6S"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8lRJpEj1i6S"
      },
      "source": [
        "#### Using positional encoding to re-inject order information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRkPKVuG1i6S"
      },
      "source": [
        "**Implementing positional embedding as a subclassed layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egafKq4Y1i6T"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRu9saNz1i6T"
      },
      "source": [
        "#### Putting it all together: A text-classification Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OFOa7S61i6T"
      },
      "source": [
        "**Combining the Transformer encoder with positional embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "260WD4cc1i6T"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Kbrk0u1i6U"
      },
      "source": [
        "### When to use sequence models over bag-of-words models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuNnNC711jN4"
      },
      "source": [
        "## 11.5. Beyond text classification: Sequence-to-sequence learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y81RhLDB1jN4"
      },
      "source": [
        "### A machine translation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy6o9Cw-1jN5"
      },
      "outputs": [],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeEKeNHT1jN6"
      },
      "outputs": [],
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQj9279b1jN6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCalmjZH1jN7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01pZRwYh1jN8"
      },
      "source": [
        "**Vectorizing the English and Spanish text pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAVUxLzm1jN8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP_-ykEj1jN9"
      },
      "source": [
        "**Preparing datasets for the translation task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tny7p8Nu1jN-"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksOTsHAL1jN_"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdFv6skf1jN_"
      },
      "source": [
        "### Sequence-to-sequence learning with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8btxa9G1jN_"
      },
      "source": [
        "**GRU-based encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdo_sPbO1jOA"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Bx2fo51jOA"
      },
      "source": [
        "**GRU-based decoder and the end-to-end model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU_CHh9M1jOB"
      },
      "outputs": [],
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIRb2PHN1jOB"
      },
      "source": [
        "**Training our recurrent sequence-to-sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkDl3JHr1jOB"
      },
      "outputs": [],
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzyNv8_B1jOC"
      },
      "source": [
        "**Translating new sentences with our RNN encoder and decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrC94E7w1jOC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSeE13ha1jOC"
      },
      "source": [
        "### Sequence-to-sequence learning with Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "femHPFaN1jOD"
      },
      "source": [
        "#### The Transformer decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcvRqQl81jOD"
      },
      "source": [
        "**The `TransformerDecoder`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLFFereu1jOD"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2TV51td1jOE"
      },
      "source": [
        "#### Putting it all together: A Transformer for machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnvrNPJA1jOE"
      },
      "source": [
        "**PositionalEmbedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4530Kvz1jOE"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4g8nMo91jOE"
      },
      "source": [
        "**End-to-end Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3whTpZfx1jOE"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyXmAIiK1jOF"
      },
      "source": [
        "**Training the sequence-to-sequence Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNUxbLW-1jOF"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZaIXJXf1jOF"
      },
      "source": [
        "**Translating new sentences with our Transformer model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HQmb9HN1jOF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    }
  ]
}