{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝_기말고사.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNJ7ITJriBAlZEcOiFLh2xU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alangkim/fchollet/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch8. Introduction to deep learning for computer vision"
      ],
      "metadata": {
        "id": "95Z4hsdS-38b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction to\n",
        "convnets\n",
        "\n",
        "2. Training a\n",
        "convnet from scratch on a small dataset\n",
        "\n",
        "3. Leveraging a\n",
        "pretrained model"
      ],
      "metadata": {
        "id": "AwpvaKrb_AAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction to convnets"
      ],
      "metadata": {
        "id": "McKuibZi_Mfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stack of Conv2D and MaxPooling2D layers"
      ],
      "metadata": {
        "id": "qRT-9fvw_VmY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk2kADnJoxUw"
      },
      "outputs": [],
      "source": [
        "# Instantiating a small convnet\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(28, 28, 1))                                     # MNIST dataset을 이용하기 위해 28*28 사용\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)     # Conv2D\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)                                     # MaxPooling2D\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)                                                     # Flatten all the information\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)                         # connect Dense layer\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)                         # making model by functional API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZZyYWIxoxUw"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOxV0KpuoxUw"
      },
      "outputs": [],
      "source": [
        "# Training the convnet on MNIST images\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)) # CNN을 이용하기 위해서 channel dimension은 필수적이다.\n",
        "# Convnet is running on the original shape of the image.\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\", # multi class classification\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the convnet\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "z-wybeXvAIJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The convolution operation"
      ],
      "metadata": {
        "id": "dtZv-RvYrmcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 'Dense layers' learn 'global patterns' in their input feature space whereas 'convolution layers' learn 'local patterns'\n",
        "\n",
        "* The patterns they learn are\n",
        "translation invariant\n",
        "\n",
        "* They can learn spatial hierarchies of patterns\n",
        "\n",
        "* Convolution preserves the spatial relationship between pixels by learning image\n",
        "features using small squares (depending on the filter size) of input data\n",
        "\n",
        "* Convolution: multiplying elementwise by filter and summing the multiplication\n",
        "outputs\n",
        "\n",
        "* Ex) a 3x3 kernel or 3x3x1 filter acts on a 5x6 input image with stride 1 and outputs\n",
        "a 3x4 feature map.\n",
        "\n",
        "* In fully connected sense, we need unshared 30(=5x6)x12(=3x4) weights (input size x output size)\n",
        "\n",
        "* 9 vs 360. So using convolution filter is far more efficient."
      ],
      "metadata": {
        "id": "3ouJ_NL9rrh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution on MxNx3 image with 3x3x3 filter producing 1 feature map by taking dot products between the filter and 3x3x3 piecies of the image.\n",
        "\n",
        "Depth part is decided based on the input feature map."
      ],
      "metadata": {
        "id": "7hZiPhQpaxi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why convolution?"
      ],
      "metadata": {
        "id": "DEi2OSthb5ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fully Connected -> 1000x1000 images, 10000 hidden nodes, 10^10 parameters\n",
        "* Convolution     -> 1000x1000 images, 10x10 filter size, 100 filters, 10^4 parameters\n",
        "\n",
        "* If you are dealing with image dataset, it's highly recommend to use convolution layers in modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "my-gSQFccLmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How convolution filter works?"
      ],
      "metadata": {
        "id": "r7Tjwn0OeVh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different values of the filter matrix produce different\n",
        "feature maps for the same input image.\n",
        "\n",
        "CNN learns the values of filters during training\n",
        "\n",
        "The more filters, the more features are extracted"
      ],
      "metadata": {
        "id": "jFMAcI2_eHt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature map"
      ],
      "metadata": {
        "id": "TtSwIzOsXFCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4 parameters of feature map\n",
        "\n",
        "1. filter size\n",
        "2. depth\n",
        "3. stride\n",
        "4. zero-padding"
      ],
      "metadata": {
        "id": "zL5fMh7yXVBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The max pooling operation"
      ],
      "metadata": {
        "id": "eXn7q_J9XsfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Role\n",
        "of max pooling: to aggressively downsample feature maps\n",
        "\n",
        "Transformed via a hardcoded max\n",
        "tensor operation\n",
        "\n",
        "We need the features from the last\n",
        "convolution layer to contain\n",
        "information about the totality of the\n",
        "input\n",
        "\n",
        "The final feature map has 22\n",
        "× 22 ×\n",
        "128 = 61,952 total coefficients per\n",
        "sample\n",
        "\n",
        "This is far too large for such a\n",
        "small model and would result in\n",
        "intense overfitting"
      ],
      "metadata": {
        "id": "gVJW72urXwjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max-pooling이 없는 경우\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "j3p6KvfpAIFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool.summary()\n",
        "# 모델의 크기에 비해 parameters가 너무 많다."
      ],
      "metadata": {
        "id": "vcWDvOrWAIC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max-pooling은 없지만 stride를 2로 지정한 경우\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")(inputs) # stride = 2 로 지정.\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "QoaoW4dZAH8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool.summary()\n",
        "# parameters가 많이 줄어들었으나 max-pooling의 결과가 더 좋다.\n",
        "# 일반적으로 classification에서는 stride보다 max-pooling을 자주 사용한다.\n",
        "# 경험적으로 대부분 average-pooling보다 max-poolng이 좋다."
      ],
      "metadata": {
        "id": "NHP6j9KlAHr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training a convnet from scratch on a small dataset"
      ],
      "metadata": {
        "id": "gyYyyqIkhznz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading a\n",
        "Kaggle dataset in Google Colaboratory\n",
        "\n",
        "Access to the API is restricted to\n",
        "Kaggle users, you need to authenticate yourself.\n",
        "\n",
        "The\n",
        "kaggle package will look for your login credentials in a JSON file located at\n",
        "kaggle kaggle.json\n",
        "\n",
        "First, you need to create a\n",
        "Kaggle API key and download it to your local machine\n",
        "Login\n",
        "--> My Account --> Account settings --> API\n",
        "Click the Create New API Token\n",
        "button\n",
        "\n",
        "\n",
        "Second, go to your\n",
        "Colab notebook, and upload the API’s key JSON file to your\n",
        "Colab session by running the following code in a notebook cell:"
      ],
      "metadata": {
        "id": "PHjhFVBMh78x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러오기"
      ],
      "metadata": {
        "id": "LXeOrQMAu8M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "7yLglkdUh2AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "rpLHAqo7ioCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "metadata": {
        "id": "URLInUr9iyPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "Aoz3fYDht2G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq dogs-vs-cats.zip"
      ],
      "metadata": {
        "id": "WPYiVZdhuQeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "o4SpUBbXuhnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq train.zip"
      ],
      "metadata": {
        "id": "P7rI26zci60T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "W3c9kqftgHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('train')"
      ],
      "metadata": {
        "id": "TYgR2-MOurIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copying images to training, validation, and test directories"
      ],
      "metadata": {
        "id": "8f_Gm9WJvF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "복잡하게 나열되어있는 data를 train, validation, test로 나누고 각각 1000개, 500개, 1000개의 data를 넣는 전처리"
      ],
      "metadata": {
        "id": "biCIyvK-vQ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "# original dataset이 풀려있는 directory\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "# smaller dataset을 저장할 directory\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        # 새로운 directory 만들기 ex) cats_vs_dogs_small/train/dog\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        # 파일 이름 만들기\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "            # src : source, dst : destination\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "# 처음 1000개로 train set을 만듦\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "# 그 다음 500개로 validation set을 만듦\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)\n",
        "# 그 다음 1000개로 test set을 만듦"
      ],
      "metadata": {
        "id": "eqRdeLOwi9um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(new_base_dir)"
      ],
      "metadata": {
        "id": "7HQHyI4WgYm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 코드와 동일\n",
        "os.listdir('cats_vs_dogs_small')"
      ],
      "metadata": {
        "id": "wEcojIOcg5-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('cats_vs_dogs_small/test')"
      ],
      "metadata": {
        "id": "ukgcOCRGgmC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('cats_vs_dogs_small/test/dog')\n",
        "# 1500~2500 index를 가진 dog 파일이 들어가있음"
      ],
      "metadata": {
        "id": "V4YvE_GNgxoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "# 180x180 size를 가진 RGB image\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "# rescale\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "# binary classification이라 activation은 sigmoid\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "JL_OwU5yjMLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "# height, width는 점점 작아지고 depth는 점점 깊어진다."
      ],
      "metadata": {
        "id": "rIvgE6VPlOR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "8LhIZc3glOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "aKdMXtiy1bT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the picture files.\n",
        "2. Decode the JPEG content to RGB grids of pixels\n",
        "3. Convert these into floating\n",
        "point tensors\n",
        "4. Resize them to a shared size (we’ll use 180\n",
        "× 180)\n",
        "5. Pack them into batches (we’ll use batches of 32 images)"
      ],
      "metadata": {
        "id": "bYb-qtb11dsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using image_dataset_from_directory to read images\n",
        "\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "metadata": {
        "id": "52IhG83OlOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gsY75MeerfoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "dkUCyI3srgi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding TensorFlow Dataset objects\n",
        "\n"
      ],
      "metadata": {
        "id": "yg8USm1Lc9Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow\n",
        "makes available the tf.data API to create efficient input pipelines\n",
        "\n",
        "The Dataset class handles many key features that would otherwise be\n",
        "cumbersome to implement yourself in particular, asynchronous data prefetching\n",
        "\n",
        "The Dataset class also exposes a functional\n",
        "style API for modifying datasets"
      ],
      "metadata": {
        "id": "e-9vJqpqbv7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
        "# from_tensor_slices() class can be used to create a Dataset from a NumPy array"
      ],
      "metadata": {
        "id": "DXMr-J0FlOId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Yielding single samples\n",
        "\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "65M3ew9_lN64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use .batch() method to batch the data\n",
        "\n",
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "zA10laH5lNxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CciDXj1NrVFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Range of useful dataset methods"
      ],
      "metadata": {
        "id": "m2vOuSd2c7p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* .shuffle(buffer_size) : Shuffles elements within a buffer\n",
        "* .prefetch (buffer_size) : Prefetches a buffer of elements in GPU memory to achieve\n",
        "better device utilization.\n",
        "* .map(callable) : Applies an arbitrary transformation to each element of the dataset"
      ],
      "metadata": {
        "id": "2VgTh1HCdH6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "xm_k1z_vli0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4))).batch(32)\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "U573UPfirnm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다시 원래 문제로 돌아가자"
      ],
      "metadata": {
        "id": "Yc82boa8rmQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the shapes of the data and labels yielded by the Dataset\n",
        "\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "BjXDskQjliyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model using a Dataset\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "C_PO7nMpliwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying curves of loss and accuracy during training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nwu2zHgQliry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "# sample이 2000개로 너무 적어 overfitting이 나타날 것이다.\n",
        "\n",
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "nvd8QAuUlioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using data augmentation to prevent overfitting"
      ],
      "metadata": {
        "id": "6lM2julrsgev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Data augmentation**\n",
        "takes the approach of generating more training data\n",
        "from existing training samples by **augmenting the samples via a number of random transformations**\n",
        "that yield believable looking images\n",
        "\n",
        "* In\n",
        "Keras , this can be done by adding a number of data augmentation layers at\n",
        "the start of your model."
      ],
      "metadata": {
        "id": "vr8eGW4Usrkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델에 다음과 같이 data_augmentation을 삽입할 수 있다.\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "vGcKdJdilikc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomFlip**(\"horizontal\")\n",
        "is for randomly flipping half the images horizontally\n",
        "\n",
        "**RandomRotation**(0.1)\n",
        "Rotates the input images by a random value in the range [ -10%, +10%]\n",
        "\n",
        "**RandomZoom**(0.2)\n",
        "Zooms in or out of the image by a random factor in the range [ -20%, +20%]"
      ],
      "metadata": {
        "id": "yMR_X34atthR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "# We can use .take(N) to only sample N batches from the dataset. This is equivalent to inserting a break in the loop after the Nth batch\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        # apply the augmentation\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        # Display the first image in the output batch.\n",
        "        # For each of the 9 iteration, this is a different augmentation of the same image\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "# augmentation을 통해 dataset이 많아지면 overfitting을 prevent할 수 있다."
      ],
      "metadata": {
        "id": "f6vTPRaUliht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a new convnet"
      ],
      "metadata": {
        "id": "qoS0XZ4XwCN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New convnet includes Image augmentation and dropout\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) # augmentation\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x) # dropout\n",
        "# dropout을 convolution layer에 사용하는 것은 좋지 않다.\n",
        "# 일반적인 Dropout은 convolution layer에 사용하지 않는다.\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Y-t0bWKtlifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the regularized convnet\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "Gt_m708FlidS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "\n",
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "# dropout과 augmentation이 없는 것보다 결과가 훨씬 좋다."
      ],
      "metadata": {
        "id": "WzoggAk7l2rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Leveraging a pretrained model"
      ],
      "metadata": {
        "id": "h1lnd-ZgmDaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A common and highly effective approach to deep learning on small image datasets\n",
        "is to use a pretrained model\n",
        "\n",
        "* **Pretrained network** is a saved network that was previously trained on a large\n",
        "dataset\n",
        "\n",
        "* Motivations:\n",
        "\n",
        "    Lots of data, time, resources needed to train and tune a neural network from\n",
        "scratch\n",
        "\n",
        "    Cheaper, faster way of adapting a neural network by exploiting their\n",
        "generalization properties"
      ],
      "metadata": {
        "id": "UYQKQ9s2oJd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Take top performing pre-trained networks(convolutional base)\n",
        "2. If we have small amount of data\n",
        "\n",
        "    Freeze all Networks + New softmax layer for cats and dogs\n",
        "\n",
        "    Training에 New softmax layer for cats and dogs만 사용한다.\n",
        "\n",
        "3. If we have larger data\n",
        "\n",
        "    Freeze some Networks + New softmax layer for cats and dogs\n",
        "\n",
        "    Training에 top performing pre-trained networks의 일부도 사용한다."
      ],
      "metadata": {
        "id": "ZgR6QxXv0zwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* List of image classification models (all pretrained on the ImageNet dataset) that are available as part of keras : Xception\n",
        ", Inception V3, ResNet50, VGG16, VGG19, MobileNet\n",
        "\n",
        "* More available from\n",
        "tensorflow hub"
      ],
      "metadata": {
        "id": "RRVXH4d9oZ6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the VGG16 convolutional base\n",
        "\n",
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False, # classifier part는 제외하고 convolutional base만 가져온다.\n",
        "    input_shape=(180, 180, 3))"
      ],
      "metadata": {
        "id": "jYOtgIdWmCpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "wu4Dv1svl2ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast feature extraction without data augmentation"
      ],
      "metadata": {
        "id": "H-3eH5tjolui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll start by extracting features as\n",
        "NumPy arrays by calling the predict()\n",
        "method of the conv_base model on our training"
      ],
      "metadata": {
        "id": "dc1xGbMLoqyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the VGG16 features and corresponding labels\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        # vgg16 pretrained network\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
        "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels =  get_features_and_labels(test_dataset)"
      ],
      "metadata": {
        "id": "qJ0GhoWgl2kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "metadata": {
        "id": "8TwhpaNul2gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining and training the densely connected classifier\n",
        "# add last layer\n",
        "# training is very fast because we only have to deal with two dense layers\n",
        "\n",
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels),\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "b2W9MrB3l2dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 2 dense layer만 사용했음에도 불구하고 결과가 좋다."
      ],
      "metadata": {
        "id": "SDCLRzWsl2Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast feature extraction with data augmentation"
      ],
      "metadata": {
        "id": "RtZDjmmwpPia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new model that chains together: \n",
        "\n",
        "1) data augmentation\n",
        "\n",
        "2) freezing convolutional base\n",
        "\n",
        "3) a dense classifier"
      ],
      "metadata": {
        "id": "g3Efjb6f7C1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating and freezing the VGG16 convolutional base\n",
        "\n",
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False) # only get convolutional base part\n",
        "conv_base.trainable = False # conv_base는 이미 잘 훈련되어있는거라 훈련시키지 않는다."
      ],
      "metadata": {
        "id": "YpREdN28pR2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the list of trainable weights before and after freezing"
      ],
      "metadata": {
        "id": "f1ZEskbn70Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "CP0pb0ubpRzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "OfKvrDgJpRwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a data augmentation stage and a classifier to the convolutional base\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) # apply data augmentation\n",
        "x = keras.applications.vgg16.preprocess_input(x) # apply input value scaling\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "SjGbraPwpRuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "-RfzSoeGpRrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "\n",
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "# 이전보다 결과가 아주 조금 좋아졌다."
      ],
      "metadata": {
        "id": "2RPzEf-vpRm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning a pretrained model"
      ],
      "metadata": {
        "id": "Lui-8FWqpn8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine\n",
        "tuning consists of unfreezing a few of the top\n",
        "layers of a frozen model base used for feature\n",
        "extraction, and jointly training both the newly added\n",
        "part of the model"
      ],
      "metadata": {
        "id": "r_lCgAK1ptpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "last convolution block을 unfreeze하고 같이 훈련시키다."
      ],
      "metadata": {
        "id": "VQO-WhqAFB2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### step"
      ],
      "metadata": {
        "id": "6xtvv4D2FdTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Add your custom network on top of an already\n",
        "trained base network\n",
        "2. Freeze the base network\n",
        "3. Train the part you added\n",
        "4. Unfreeze some layers in the base network\n",
        "5. Jointly train both these layers and the part you added"
      ],
      "metadata": {
        "id": "5JkTahvoFm55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing all layers until the fourth from the last\n",
        "\n",
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "dRCOE-cIpRjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "              # we use smaller lr\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"fine_tuning.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "zn9q-H4Cl2Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "# Many times it will improve the results"
      ],
      "metadata": {
        "id": "NUR6PrerGOKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Convnets\n",
        "are the best type of machine learning models for\n",
        "computer vision\n",
        "2. On a small dataset, overfitting will be the main issue. Data\n",
        "augmentation is a powerful way\n",
        "3. It’s easy to reuse an existing\n",
        "convnet on a new dataset via\n",
        "transfer learning\n",
        "4. As a complement to feature extraction, you can use fine\n",
        "tuning"
      ],
      "metadata": {
        "id": "qkbR-vfuHXxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch9. Advanced deep learning for computer vision"
      ],
      "metadata": {
        "id": "Il07z1u_M8h6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Three essential computer vision tasks\n",
        "2. An image segmentation example\n",
        "3. Modern\n",
        "convnet architecture patterns\n",
        "4. Interpreting what\n",
        "convnets learn"
      ],
      "metadata": {
        "id": "5HX45s0ANOPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1. Three essential computer vision tasks"
      ],
      "metadata": {
        "id": "5j-PtbCINYxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Image classification**\n",
        ": assign one or\n",
        "more labels to an image\n",
        "2. **Image segmentation**\n",
        ": goal is to\n",
        "“segment” or “partition” an image into\n",
        "different areas, with each area usually\n",
        "representing a category\n",
        "3. **Object detection**\n",
        ": goal is to draw\n",
        "rectangles (called bounding boxes)\n",
        "around objects of interest in an image,\n",
        "and associate each rectangle with a"
      ],
      "metadata": {
        "id": "BOLZLOM4NgNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2. Image segmentation example"
      ],
      "metadata": {
        "id": "ihFWVPILN5gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image segmentation with deep learning is about using a model to assign a class\n",
        "to each pixel in an image (such as “background” and “foreground,” or “road,”\n",
        "“car,” and “sidewalk\"\n",
        "\n",
        "* **Semantic segmentation**, where each pixel is independently classified into a\n",
        "semantic category\n",
        "\n",
        "* **Instance segmentation**, which seeks not only to classify image pixels by\n",
        "category, but also to parse out individual object instances"
      ],
      "metadata": {
        "id": "bQNIyY1VN5DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oxford IIIT Pets dataset"
      ],
      "metadata": {
        "id": "JbU3KeiDO97f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contains 7,390 pictures of various breeds of cats and dogs, together with\n",
        "foreground background segmentation masks\n",
        "\n",
        "**Segmentation mask**\n",
        "is the image segmentation equivalent of a label: it’s an\n",
        "image the same size as the input image, with a single color channel where each\n",
        "integer value corresponds to the class: 1 (foreground), 2 (background), and\n",
        "3(contour)"
      ],
      "metadata": {
        "id": "PC4gL2H2PNJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz\n",
        "\n",
        "# !wget : download file from the website\n",
        "# !tar : unzip file"
      ],
      "metadata": {
        "id": "Y_AtguN4NNRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory 안에 있는 file 확인\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "xRBBlZB3QXAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory 안에 있는 file 확인\n",
        "\n",
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "K3uovbhXQxU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('Images')"
      ],
      "metadata": {
        "id": "vfQ_B-pPQ40x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnms1 = os.listdir('Images')\n",
        "len(fnms1)"
      ],
      "metadata": {
        "id": "ZvDv5EBnQ-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('annotations')\n",
        "# annotation : 주석"
      ],
      "metadata": {
        "id": "rWxXInomROyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat annotations/README"
      ],
      "metadata": {
        "id": "Pxk7Vcb2RYk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('annotations/trimaps/')"
      ],
      "metadata": {
        "id": "Lm5fJVsjRpQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnms2 = os.listdir('annotations/trimaps/')\n",
        "len(fnms2)\n",
        "# fnms1보다 크다 : 중복 파일이 존재한다는 의미"
      ],
      "metadata": {
        "id": "4fmGKalmRxjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [os.path.join(input_dir, fname)     # join해라\n",
        "     for fname in os.listdir(input_dir) # input_dir에 있는 fname을\n",
        "     if fname.endswith(\".jpg\")])        # fname이 .jpg로 끝나면\n",
        "\n",
        "target_paths = sorted(\n",
        "    [os.path.join(target_dir, \n",
        "                  fname)\n",
        "     for fname in os.listdir(target_dir)\n",
        "     if fname.endswith(\".png\") and not fname.startswith(\".\")]) # 중복 파일 제거"
      ],
      "metadata": {
        "id": "KJ0kc8GlPcDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img_paths[:5]"
      ],
      "metadata": {
        "id": "kHyzwMNiWub7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_paths[:5]"
      ],
      "metadata": {
        "id": "gRw77_boXPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_img_paths)"
      ],
      "metadata": {
        "id": "badJXTzFXXI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_paths)\n",
        "# 중복 파일 제거 성공"
      ],
      "metadata": {
        "id": "5ZC15vNJXaML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10번째 이미지\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(load_img(input_img_paths[9]))"
      ],
      "metadata": {
        "id": "3vxwl0PfYQJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# annotation\n",
        "\n",
        "def display_target(target_array):\n",
        "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(normalized_array[:, :, 0])\n",
        "\n",
        "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
        "display_target(img)"
      ],
      "metadata": {
        "id": "8LbXMDX9YQIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our inputs and targets into two NumPy arrays\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "img_size = (200, 200)\n",
        "# resize everything\n",
        "num_imgs = len(input_img_paths)\n",
        "# total number of samples in the data\n",
        "\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_paths)\n",
        "# seed number를 1337로 동일하게 지정해줘서 input과 target이 same order를 가지면서 shuffle 될 수 있다.\n",
        "\n",
        "def path_to_input_image(path):\n",
        "    return img_to_array(load_img(path, target_size=img_size))\n",
        "\n",
        "def path_to_target(path):\n",
        "    img = img_to_array(\n",
        "        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
        "    img = img.astype(\"uint8\") - 1\n",
        "    return img\n",
        "\n",
        "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
        "# (num_imgs,)는 7000, img_size는 위에서 resize한 대로 (200, 200), RGB라서 (3,)\n",
        "# 따라서 결론적으로 (7000, 200, 200, 3)\n",
        "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
        "# (7000, 200, 200, 1)\n",
        "# 마지막 1은 1 or 2 or 3 셋 중에 한 숫자가 들어감\n",
        "for i in range(num_imgs):\n",
        "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
        "    targets[i] = path_to_target(target_paths[i])\n",
        "\n",
        "# validation을 위한 1000개의 sample\n",
        "num_val_samples = 1000\n",
        "\n",
        "# split the data into training and validation\n",
        "train_input_imgs = input_imgs[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_input_imgs = input_imgs[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]"
      ],
      "metadata": {
        "id": "LxzrhyluYQGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_imgs.shape"
      ],
      "metadata": {
        "id": "2ABWFPX9sJsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets.shape"
      ],
      "metadata": {
        "id": "-89etlkXsMvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modeling\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,)) # (200, 200, 3)\n",
        "    x = layers.Rescaling(1./255)(inputs) # rescale\n",
        "\n",
        "    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    # maxpooling을 사용하지 않고 stride 사용\n",
        "\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-CiXU5K_YQEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(img_size=img_size, num_classes=3)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uLzlsYcvYQCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The first half\n",
        "of the model closely resembles the kind of\n",
        "convnet you’d use for image classification\n",
        "\n",
        "Encode the images into smaller feature maps that contain\n",
        "spatial information about original image\n",
        "\n",
        "Downsample\n",
        "by adding strides rather than using\n",
        "maxpooling because we care a lot about the spatial location\n",
        "of information, **maxpooling destroy location information** (stride는 spatial location information이 남아있다.)"
      ],
      "metadata": {
        "id": "_bkiuR5Hbpk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The second half\n",
        "of the model is a stack of\n",
        "Conv2DTranspose layers, inverse of the transformations\n",
        "\n",
        "Transformation going in the opposite direction of\n",
        "convolutions"
      ],
      "metadata": {
        "id": "MdwnlDPpcaOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Up sampling\n",
        "\n",
        "Motivation : Need a transformation going in the opposite direction of convolutions\n",
        "\n",
        "* Generating images involving up sampling from low resolution to high resolution\n",
        "\n",
        "* Decoding layer of a convolutional auto encoder\n",
        "\n",
        "Neural network up\n",
        "samplings: Transposed convolution, Fractionally strided\n",
        "convolution"
      ],
      "metadata": {
        "id": "B_dOuKOkqG1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transposed convolution"
      ],
      "metadata": {
        "id": "J2oiOOSlqg1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Going backward of a convolution operation such that it has the similar positional\n",
        "connectivity and forms a one to many relationship\n",
        "\n",
        "* We can express a convolution\n",
        "operation using a convolution\n",
        "matrix, which is nothing but a\n",
        "rearranged matrix\n",
        "\n",
        "* We similarly express a transposed\n",
        "convolution using a transposed\n",
        "convolution matrix, whose layout is\n",
        "a transposed shape but in which\n",
        "the actual weight values does not\n",
        "have to come from the original\n",
        "convolution matrix"
      ],
      "metadata": {
        "id": "KzwCfR2Vq5Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and fit\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "# 원 핫 인코딩을 한다면 loss에 categorical_crossentropy도 사용 가능\n",
        "# 현재는 targets이 0, 1, 2의 값을 갖기 때문에 sparse_categorical_crossentropy 사용\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_input_imgs, train_targets,\n",
        "                    epochs=50,\n",
        "                    callbacks=callbacks,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(val_input_imgs, val_targets))"
      ],
      "metadata": {
        "id": "ulrkr1DZYP_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "EV3-71XVYP-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload our best performing model according to the validation loss,\n",
        "and demonstrate how to use it to predict a segmentation mask"
      ],
      "metadata": {
        "id": "Cr2AIPUIr0l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import array_to_img\n",
        "\n",
        "model = keras.models.load_model(\"oxford_segmentation.keras\")\n",
        "\n",
        "i = 4\n",
        "test_image = val_input_imgs[i]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(array_to_img(test_image))\n",
        "\n",
        "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
        "\n",
        "def display_mask(pred):\n",
        "    mask = np.argmax(pred, axis=-1)\n",
        "    mask *= 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(mask)\n",
        "\n",
        "display_mask(mask)"
      ],
      "metadata": {
        "id": "miNvJBZnYP7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3 Modern convnet architecture patterns"
      ],
      "metadata": {
        "id": "BhaiOFQ8tHqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good model architecture is one that\n",
        "reduces the size of the search space or\n",
        "otherwise makes it easier to converge to a good point of the search space\n",
        "\n",
        "Model architecture is more an art than a science. Experienced machine learning\n",
        "engineers are able to\n",
        "intuitively cobble together high performing models on\n",
        "their first try, while beginners often struggle to create a model that trains at all\n",
        "\n",
        "You’ll develop your own\n",
        "intuition throughout this book\n",
        "\n",
        "In the following sections, we’ll review a few essential\n",
        "convnet architecture best\n",
        "practices:\n",
        "**residual connections , batch normalization , and separable convolutions**\n",
        "\n",
        "We will apply them to our cat vs. dog classification problem"
      ],
      "metadata": {
        "id": "IQo2vtxvtHYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rdsidual connections"
      ],
      "metadata": {
        "id": "sDNnYyBPzKMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MZRoQ84YtU8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}